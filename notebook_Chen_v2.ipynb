{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteo Click Through Rate Prediction \n",
    "\n",
    "### W261 - Machine Learning at Scale \n",
    "Spring Semester  \n",
    "Ben Arnoldy, Kenneth Chen, Nick Conidas, Rohini Kashibatla, Pavan Kurapati\n",
    "\n",
    "Criteo is an advertising company that specializes in web ad and gathered click through information via their ad services. In 2014, Criteo launched Click Through Rate (CTR) prediction competition hosted in Kaggle. It provides train.txt, test.txt where train.txt was provided with its label: `1` for click, and `0` for no-click. The test.txt was given with a number of features without labels for which we have to predict whether the user clicks the web ad or not. \n",
    "\n",
    "### Dataset construction:\n",
    "\n",
    "The training dataset consists of a portion of Criteo's traffic over a period\n",
    "of 7 days. Each row corresponds to a display ad served by Criteo and the first\n",
    "column is indicates whether this ad has been clicked or not.\n",
    "The positive (clicked) and negatives (non-clicked) examples have both been\n",
    "subsampled (but at different rates) in order to reduce the dataset size.\n",
    "\n",
    "There are 13 features taking integer values (mostly count features) and 26\n",
    "categorical features. The values of the categorical features have been hashed\n",
    "onto 32 bits for anonymization purposes. \n",
    "The semantic of these features is undisclosed. Some features may have missing values.\n",
    "\n",
    "The rows are chronologically ordered.\n",
    "\n",
    "The test set is computed in the same way as the training set but it \n",
    "corresponds to events on the day following the training period. \n",
    "The first column (label) has been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubrics \n",
    "## Question 1: Question Formulation\n",
    "Introduce the goal of your analysis. What questions will you seek to answer, why do people perform this kind of analysis on this kind of data? Preview what level of performance your model would need to achieve to be practically useful.\n",
    "\n",
    "## Question 2: Algorithm Explanation\n",
    "Create your own toy example that matches the dataset provided and use this toy example to explain the math behind the algorithym that you will perform.\n",
    "\n",
    "## Question 3: EDA & Discussion of Challenges\n",
    "Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable. Discuss any challenges that you anticipate based on the EDA you perform\n",
    "\n",
    "## Question 4: Algorithm Implementation\n",
    "Develop a 'homegrown' implementation of the algorithn, apply it to the training dataset and evaluate your results on the test set.\n",
    "\n",
    "## Question 5: Application of Course Concepts\n",
    "Pick 3-5 key course concepts and discuss how your work on this assignment illustrates an understanding of these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw5_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into Spark RDDs for convenience of use later (RUN THIS CELL AS IS)\n",
    "trainRDD = sc.textFile('data/train.txt')\n",
    "testRDD = sc.textFile('data/test.txt')\n",
    "sampleRDD = sc.textFile('data/dac_sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t5\t0\t1382\t4\t15\t2\t181\t1\t2\t\t2\t68fd1e64\t80e26c9b\tfb936136\t7b4723c4\t25c83c98\t7e0ccccf\tde7995b8\t1f89b562\ta73ee510\ta8cd5504\tb2cb9c98\t37c9c164\t2824a5f6\t1adce6ef\t8ba8b39a\t891b62e7\te5ba7672\tf54016b9\t21ddcdc9\tb1252a9d\t07b5194c\t\t3a171ecb\tc5c50484\te8b83407\t9727dd16\r\n",
      "0\t2\t0\t44\t1\t102\t8\t2\t2\t4\t1\t1\t\t4\t68fd1e64\tf0cf0024\t6f67f7e5\t41274cd7\t25c83c98\tfe6b92e5\t922afcc0\t0b153874\ta73ee510\t2b53e5fb\t4f1b46f3\t623049e6\td7020589\tb28479f6\te6c5b5cd\tc92f3b61\t07c540c4\tb04e4670\t21ddcdc9\t5840adea\t60f6221e\t\t3a171ecb\t43f13e8b\te8b83407\t731c3655\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t29\t50\t5\t7260\t437\t1\t4\t14\t\t1\t0\t6\t5a9ed9b0\ta0e12995\ta1e14474\t08a40877\t25c83c98\t\t964d1fdd\t5b392875\ta73ee510\tde89c3d2\t59cd5ae7\t8d98db20\t8b216f7b\t1adce6ef\t78c64a1d\t3ecdadf7\t3486227d\t1616f155\t21ddcdc9\t5840adea\t2c277e62\t\t423fab69\t54c91918\t9b3e8820\te75c9ae9\r\n",
      "27\t17\t45\t28\t2\t28\t27\t29\t28\t1\t1\t\t23\t68fd1e64\t960c983b\t9fbfbfd5\t38c11726\t25c83c98\t7e0ccccf\tfe06fd10\t062b5529\ta73ee510\tca53fc84\t67360210\t895d8bbb\t4f8e2224\tf862f261\tb4cc2435\t4c0041e5\te5ba7672\tb4abdd09\t21ddcdc9\t5840adea\t36a7ab86\t\t32c7478e\t85e4d73f\t010f6491\tee63dd9b\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45840617 data/train.txt\n",
      "  100000 data/dac_sample.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/train.txt\n",
    "!wc -l data/dac_sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Question Formulation\n",
    "Introduce the goal of your analysis. What questions will you seek to answer, why do people perform this kind of analysis on this kind of data? Preview what level of performance your model would need to achieve to be practically useful.\n",
    "\n",
    "## Objectives \n",
    "\n",
    "Given the click through dataset, our objective is to accurately predict whether or not the display ad will be clicked. Since the majority of features are categorical in nature and the values are hashed to keep the privacy, our main goal is to engineer features based on our model performance. There are two caveates in designing features engineering and executing exploratory data analysis (EDA). \n",
    "\n",
    "1. The dataset is extremely large with more than 45 million samples collected over 7 days. In order to process a large data, we can either subtract a small sample set and execute EDA or design an algorithm that offers a parallel computation in the first place. For this caution, we will do both in our EDAs. We will analyze on a small dataset in terms of each features correlation and their distribution or histogram. We'd also implement an algorithm in Spark that allows parallel computation on a huge amount of data. \n",
    "\n",
    "2. The second caveate is the categorical features in our dataset. In principle, most of the python libraries offer to impute the missing value, either by taking the mean or median of the feature value in which missing values exist. However, in tackling with the large dataset in Spark and majority of features being the categorical in nature, Spark does not have any imputer function for categorical variable. <a href=\"https://spark.apache.org/docs/2.2.0/ml-features.html\">Ref</a> \n",
    "\n",
    "We will go through step by step as to how we would explore our data and design an algorithm to implement in Spark in the following sessions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Algorithm Explanation\n",
    "\n",
    "This is the fundamental concept in feature engineering when exploring our Criteo dataset. \n",
    "\n",
    "## Data Exploration \n",
    "\n",
    "```\n",
    "  45,840,617 (Criteo dataset in Kaggle) \n",
    "      |\n",
    "      |\n",
    "   100,000 (sample) \n",
    "    |-- train data      : 80,000 (80%) \n",
    "    |-- validation data : 10,000 (10%)\n",
    "    |-- test data       : 10,000 (10%)\n",
    "```\n",
    "\n",
    "Initially, we will explore 100,000 sample dataset out of original 45 million samples. To test the performance of our classifiers, we split 80% of the sample dataset into train data (80,000), 10% into validation data (10,000) and the remaining 10% for the final test data (10,000). \n",
    "\n",
    "## Features Engineering (One Hot Encoding Vs Features Hashing) \n",
    "\n",
    "Each sample has `40` variables. The first variable or the column is the label: `1` or `0` to indicate the clicking event, `1` being the clicked activity. The remaining `39` variables are features, of which `13` are represented in numeric value and the rest `26` are represented in hashed value. Some of them have missing values. All features are categorical features. In order to capture all the features, we will explore the most common feature engineering methods for categorical variables: **one hot encoding (OHE)** and **feature hashing**. \n",
    "\n",
    "### 1. One Hot Encoding (OHE)\n",
    "\n",
    "As the name suggests, one hot encoding (OHE) expands all the unique features in the dataset. \n",
    "\n",
    "```\n",
    "|         | feature1 | feature2 | feature3 | \n",
    "|---------|----------|----------|----------|\n",
    "|sample 1 | black    | round    | matte    |\n",
    "|sample 2 | white    | square   | shiny    |\n",
    "|sample 3 | blue     | round    | matte    |\n",
    "|sample 4 | black    | round    | shiny    | \n",
    "\n",
    "\n",
    "|         |black |white |blue ||round |square ||matte  |shiny | \n",
    "|---------|------|------|-----||------|-------||-------|------|\n",
    "|sample 1 | 1    | 0    | 0   || 1    |  0    || 1     | 0    |  \n",
    "|sample 2 | 0    | 1    | 0   || 0    |  1    || 0     | 1    |\n",
    "|sample 3 | 0    | 0    | 1   || 1    |  0    || 1     | 0    | \n",
    "|sample 4 | 1    | 0    | 0   || 1    |  0    || 0     | 1    | \n",
    "\n",
    "\n",
    "index    = [0, 1, 2, 3, 4, 5, 6]\n",
    "--------------------------------\n",
    "sample 1 = [1, 0, 0, 1, 0, 1, 0] \n",
    "sample 2 = [0, 1, 0, 0, 1, 0, 1]\n",
    "sample 3 = [0, 0, 1, 1, 0, 1, 0]\n",
    "sample 4 = [1, 0, 0, 1, 0, 0, 1]\n",
    "```\n",
    "\n",
    "The advantage of one hot encoding is when the dataset is relatively small, it is convenient to capture all the unique features. We can also create a sparse vector to represent our dataset in more compact fashion. However one hot encoding becomes computationaly expensive when the dataset is extremely large, for eg, our Criteo Kaggle dataset which comes with 45 millions of samples and there are at least 33 millions unique features. \n",
    "\n",
    "The other disadvantage of One Hot Encoding is, in order to capture all the unique features, we need to have a single pass over the entire dataset, which makes the computation more expensive for the large data. However we will use OHE in our exploratory data analysis and compare with features hashing method described below. \n",
    "\n",
    "\n",
    "### 2. Features Hashing \n",
    "\n",
    "Features hashing is one of the feature engineering methods that is extremely powerful in handling large data. It also offers a unique trait known as **\"online learning\"** in which the model does not need to be trained all over again due to a new dataset. The model can immediately learns online and update the hyperparameters as it needs. \n",
    "\n",
    "As the name implies, we will have a hash function to hash all the unique features in our dataset. Hash function itself is based on the modulo function, we will need to give an ample amount of hash indexes in the beginning in order to capture all **unique** features. \n",
    "\n",
    "```\n",
    "|         | feature1 | feature2 | feature3 | \n",
    "|---------|----------|----------|----------|\n",
    "|sample 1 | black    | round    | matte    |\n",
    "|sample 2 | white    | square   | shiny    |\n",
    "|sample 3 | blue     | round    | matte    |\n",
    "|sample 4 | black    | round    | shiny    | \n",
    "```\n",
    "Imagine we don't know how many unique features exists in our dataset. But we will generously create a hash table to hold all the unique features. We will start with giving $2^3 = 8$ indexes for our dataset. \n",
    "```\n",
    "|         |    list of features    | \n",
    "|---------|------------------------|\n",
    "|sample 1 | [black, round, matte]  |  \n",
    "|sample 2 | [white, square, shiny] |\n",
    "|sample 3 | [blue, round, matte]   |\n",
    "|sample 4 | [black, round, shiny]  |\n",
    "```\n",
    "There are a number of hash value using different bits. The most common being md5 using 128 bits, we will use md5 hash function.\n",
    "```\n",
    "step 1: feature = feature.encode('utf-8')                    # feature needs to be encoded\n",
    "step 2: hashlib.md5(feature)                                 # hash object\n",
    "step 3: hashlib.md5(feature).hexdigest()                     # hash object converted to hexadecimal format \n",
    "step 4: int(hashlib.md5(feature).hexdigest(), 16)            # converting hex to base 10 \n",
    "step 5: int(int(hashlib.md5(feature).hexdigest(), 16) % 8)   # modulo to match the hash table, here 2^3 = 8\n",
    "eg    : int(int(hashlib.md5('black'.encode('utf-8')).hexdigest(), 16) % 8) \n",
    "\n",
    "|         | index list |   \n",
    "|---------|------------|\n",
    "|sample 1 | [1, 2, 3]  |  \n",
    "|sample 2 | [4, 0, 6]  |\n",
    "|sample 3 | [7, 2, 3]  |\n",
    "|sample 4 | [1, 2, 6]  |\n",
    "\n",
    "\n",
    "index    = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "-----------------------------------\n",
    "sample 1 = [0, 1, 1, 1, 0, 0, 0, 0] \n",
    "sample 2 = [1, 0, 0, 0, 1, 0, 1, 0]\n",
    "sample 3 = [0, 0, 1, 1, 0, 0, 0, 1]\n",
    "sample 4 = [0, 1, 1, 0, 0, 0, 1, 0]\n",
    "\n",
    "```\n",
    "As decribed above, feature hashing also generates one hot encoding format. By generously giving more than enough index spaces for all unique features, we avoids **hash collision** situation. Even if it occurs in a large hash table, it should not affect the accuracy of our model. \n",
    "\n",
    "The most significant advantage of using feature hashing is, we **don't** need to go over the entire dataset to convert the sample features into vector space. For eg, the first sample 1 comes with `black, round, matte` features. We don't need to know how many unique colors are there in our dataset. It could be only `black` color or more than that. But the modulo function in hash compression will take care of all the unique features and give the index based on the remainder. Similarly, our $2^3 = 8$ hash table also takes care of `round` feature as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA. Average Click Through Rate from the entire dataset (45 millions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average click through rate is :  0.25622338372976045\n",
      "Time taken : 387.6914131641388 seconds\n"
     ]
    }
   ],
   "source": [
    "# This approach takes 5.7 minutes. \n",
    "start = time.time()\n",
    "print(\"The average click through rate is : \", trainRDD.map(lambda x: int(x.split('\\t')[0])).mean())\n",
    "print(\"Time taken : {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average click through rate is 0.2562233837297609\n",
      "Time taken : 176.37068128585815 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class FloatAccumulatorParam(AccumulatorParam):\n",
    "    \"\"\"\n",
    "    Custom accumulator for use in page rank to keep track of various masses.\n",
    "    \n",
    "    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "    We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "    \"\"\"\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2\n",
    "    \n",
    "def avgCTR(dataRDD):\n",
    "    \n",
    "    clickCount = sc.accumulator(0.0)\n",
    "    totAccum = sc.accumulator(0.0)\n",
    "    \n",
    "    def countCTR(line):\n",
    "        cnt = line.split('\\t')[0]\n",
    "        clickCount.add(int(cnt))\n",
    "        totAccum.add(1)\n",
    "        \n",
    "    dataRDD.foreach(countCTR)\n",
    "    tempRDD = dataRDD.map(countCTR)\n",
    "    \n",
    "    average = clickCount.value/totAccum.value\n",
    "        \n",
    "    return average     \n",
    "\n",
    "# This approach takes 2.7 minutes. \n",
    "start = time.time()\n",
    "CTR = avgCTR(trainRDD) \n",
    "print(\"The average click through rate is {}\".format(CTR)) \n",
    "print(\"Time taken : {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "Average click through rate is `0.26` which indicates that on average a user will click 25 display ads out of 100 on the webpage. This reflects the poor performance of the display ad. Ideally, we would want near 100% click on display ads. A click through rate of at least 80% will enhance the profits of the display ads. This shows that there are some features critical to the success of the display ad clicks or could be focused on by the advertising team.  \n",
    "\n",
    "In addition, the in-line lambda function took twice as long as the accumulator function to calculate the average click through rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100,000 samples split into Train, Validation and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t5\t0\t1382\t4\t15\t2\t181\t1\t2\t\t2\t68fd1e64\t80e26c9b\tfb936136\t7b4723c4\t25c83c98\t7e0ccccf\tde7995b8\t1f89b562\ta73ee510\ta8cd5504\tb2cb9c98\t37c9c164\t2824a5f6\t1adce6ef\t8ba8b39a\t891b62e7\te5ba7672\tf54016b9\t21ddcdc9\tb1252a9d\t07b5194c\t\t3a171ecb\tc5c50484\te8b83407\t9727dd16\r\n",
      "0\t2\t0\t44\t1\t102\t8\t2\t2\t4\t1\t1\t\t4\t68fd1e64\tf0cf0024\t6f67f7e5\t41274cd7\t25c83c98\tfe6b92e5\t922afcc0\t0b153874\ta73ee510\t2b53e5fb\t4f1b46f3\t623049e6\td7020589\tb28479f6\te6c5b5cd\tc92f3b61\t07c540c4\tb04e4670\t21ddcdc9\t5840adea\t60f6221e\t\t3a171ecb\t43f13e8b\te8b83407\t731c3655\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 data/dac_sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100000 data/dac_sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/dac_sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data:  80053\n",
      "Number of val data  :  9941\n",
      "Number of test data :  10006\n",
      "number of total data:  100000\n",
      "['0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n"
     ]
    }
   ],
   "source": [
    "sampleRDD1 = sampleRDD.map(lambda x: x.replace('\\t', ','))\n",
    "\n",
    "# Splitting the data\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "TrainData, ValData, TestData = sampleRDD1.randomSplit(weights,seed)\n",
    "\n",
    "# count the data\n",
    "nTrain = TrainData.count()\n",
    "nVal = ValData.count()\n",
    "nTest = TestData.count()\n",
    "print(\"Number of train data: \", nTrain)\n",
    "print(\"Number of val data  : \", nVal)\n",
    "print(\"Number of test data : \", nTest)\n",
    "print(\"number of total data: \", nTrain + nVal + nTest)\n",
    "print(sampleRDD1.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Algorithm Implementation\n",
    "\n",
    "This is a step by step walk through on how we develop home grown algorithm with particularly a parallel computation concept in mind. Since our dataset is very large, in order to capture all the unique features, we first employed one hot encoding approach for our small dataset `100,000` samples. \n",
    "\n",
    "## Step by step walk through to create one hot encoding\n",
    "\n",
    "One sample \n",
    "```\n",
    "['0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n",
    "```\n",
    "### Step 1 (Features selection) \n",
    "\n",
    "Remove the first column, i.e., label `1` or `0`. \n",
    "\n",
    "```\n",
    "['0']\n",
    "[1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16']\n",
    "```\n",
    "\n",
    "### Step 2 (Index each feature)\n",
    "\n",
    "Assign the remaining `39` variables with corresponding indexes. This step is critical because we will then use the index as our keys and in the reducer step, we could count how many unique features exists in each index.  \n",
    "\n",
    "```\n",
    "[[(0, '1'),      # index 0, feature 1 with the category '1'\n",
    "  (1, '1'),\n",
    "  (2, '5'),\n",
    "  (3, '0'),\n",
    "  (4, '1382'), ...],\n",
    " [(0, '5'),      # index 0, feature 1 with the category '5'\n",
    "  (1, '10'), \n",
    "  (2, '34'), ...], ...,\n",
    "  []]\n",
    "```\n",
    "\n",
    "### Step 3 (Select unique) \n",
    "\n",
    "The index `0` can have many features. For example \n",
    "\n",
    "```\n",
    "(0, '1')\n",
    "(0, '5')\n",
    "(0, '15')\n",
    "(0, '8')\n",
    "(0, '1')      --> (already had above), will be removed by distinct()\n",
    "(0, '15')     --> (already had above), will be removed by distinct() \n",
    "...\n",
    "```\n",
    "To count the unique features for each index, we will use `distinct()`. Distinct() step will be expensive especially when the data is very large. \n",
    "\n",
    "### Step 4 [Optional] (Mapper and Reducer stage) \n",
    "\n",
    "1. Map every index and emit them with `1` so that we can reduce them. \n",
    "2. Reduce by (lambda x, y: x + y) \n",
    "\n",
    "```\n",
    "[(0, 145),\n",
    " (1, 2483),\n",
    " (2, 864),\n",
    " (3, 130),\n",
    " (4, 20352),\n",
    " ...\n",
    "```\n",
    "\n",
    "There are 145 unique categories for the index `0`, which is the first feature out of `39` features in our sample. \n",
    "\n",
    "### Step 4 (Create One Hot Dictionary) \n",
    "\n",
    "We need to create an index dictionary for all the unique features. Once we have the dictionary, we then go over all the samples and create a sparse vector based on what features each sample has. \n",
    "\n",
    "```\n",
    "1. zipWithIndex() will create an index for every items\n",
    "2. The emitted from above will become the tuple with the index starting from `0`.\n",
    "3. [((0, '1'), 0), ((4, '1382'), 1), ((7, '2'), 2), ((13, '68fd1e64'), 3), ((14, '80e26c9b'), 4), ...]\n",
    "4. collectAsMap() will create a dictionary by mapping the first [0] in a tuple and makes them as keys (composite key)\n",
    "5. the index becomes the dict value\n",
    "\n",
    "{(0, '1'): 0,\n",
    " (4, '1382'): 1,\n",
    " (7, '2'): 2,\n",
    " (13, '68fd1e64'): 3,\n",
    " ...,\n",
    " ...,\n",
    " (36, '9a1c7e3d'): 234358}\n",
    "```\n",
    "\n",
    "### Step 5 (One Hot Encoding) \n",
    "\n",
    "In order to pipeline our RDD into our classifiers in later steps. Spark has a LabeledPoint which requires label in its tuple index`[0]` position. the index`[1]` has the sparse vector format. Reference <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=labeledpoint\">here</a>. This is what we try to achieve from our RDD sample. \n",
    "\n",
    "```\n",
    "[LabeledPoint(0.0, (234358,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,117215,117216,117217,117218,117219,117220,117221,117222,117223,117224,117225,117226,117227,117228,117229,117230,117231,117232,117233,117234,117235],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))] \n",
    "```\n",
    "1. The first `0.0` at tuple index`[0]` is the sample label. \n",
    "2. `234358` is the length of the vector space. \n",
    "3. The first list `[]` is the index list for all the unique features for the sample. \n",
    "4. The second list `[]` is the value of the unique features. Since this is OHE, it's `1`. \n",
    "\n",
    "\n",
    "- To achieve this, we need to feed our original RDD because we need the original label for each sample. \n",
    "- To have the sparse vector, we already created OHE dictionary in Step 4. We will then create our sample into `(index, category)` format, and search in our OHE dictionary for their corresponding index.  \n",
    "\n",
    "## Step 6 (Classifiers) \n",
    "\n",
    "We will use logistic regression because our label is a binary classification, either `1` or `0`. We will use log loss function as to check our model performance. A general rule of thumb is the nearer the log loss value is to zero, the more accurate our model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def parsePoint(point):\n",
    "def indexFeatures(line):\n",
    "    #  index each feature after removing the first column which is the label\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for index,feature in enumerate(line.split(',')[1:]):\n",
    "        idxFeat = index,feature\n",
    "        features.append(idxFeat)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 145), (1, 2483), (2, 864), (3, 130), (4, 20352), (5, 1899), (6, 583), (7, 131), (8, 1803), (9, 8), (10, 84), (11, 66), (12, 254), (13, 484), (14, 495), (15, 36200), (16, 21463), (17, 134), (18, 12), (19, 7236), (20, 232), (21, 3), (22, 9863), (23, 3682), (24, 34184), (25, 2748), (26, 26), (27, 4864), (28, 28933), (29, 10), (30, 2416), (31, 1230), (32, 4), (33, 32056), (34, 10), (35, 14), (36, 10849), (37, 49), (38, 8359)]\n"
     ]
    }
   ],
   "source": [
    "indexedTrainFeat = TrainData.map(indexFeatures)\n",
    "\n",
    "totalCat = (indexedTrainFeat\n",
    "            .flatMap(lambda x: x)\n",
    "            .distinct()\n",
    "            .map(lambda x: (x[0], 1))\n",
    "            .reduceByKey(lambda x, y: x + y)\n",
    "            .sortByKey()\n",
    "            .collect())\n",
    "\n",
    "print(totalCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique categories for all features:  234358\n"
     ]
    }
   ],
   "source": [
    "# Counting total number of categories\n",
    "total = 0\n",
    "for item in totalCat:\n",
    "    total += item[1]\n",
    "print(\"Total number of unique categories for all features: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOHEDict(dataRDD):\n",
    "    \"\"\" This is an function to create one hot encoding (OHE) dictionary \n",
    "    on the entire dataset. To make sure there's a unique number of categories for each feature,\n",
    "    it will use the distinct() function. \"\"\"\n",
    "    \n",
    "    OHEDict = (dataRDD.map(indexFeatures)\n",
    "                       .flatMap(lambda x: x)\n",
    "                       .distinct()\n",
    "                       .zipWithIndex()\n",
    "                       .collectAsMap())\n",
    "    return OHEDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993\n",
      "234358\n"
     ]
    }
   ],
   "source": [
    "trainOHEDict = createOHEDict(TrainData)\n",
    "print(trainOHEDict[(15, 'f5717f7e')])\n",
    "\n",
    "trainOHEFeatures = len(trainOHEDict.keys())\n",
    "print(trainOHEFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. One Hot Encoding \n",
    "## Preparing each dataset for our classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def sparseOHE(feature_list, OHEDict):\n",
    "    \"\"\"Emitting SparseVector from the entire one hot encoding dictionary \n",
    "    for one sample data at a time\n",
    "    \"\"\"\n",
    "\n",
    "    index_list = []\n",
    "    \n",
    "    for feat in feature_list:\n",
    "        if feat in OHEDict:                     # this step is for other dataset when the unique feature might not be in the OHE dict\n",
    "            index_list.append(OHEDict[feat])\n",
    "        \n",
    "    index_list.sort()\n",
    "    \n",
    "    # create a list of 1.0s for each feature\n",
    "    values = [1.] * len(index_list)\n",
    "\n",
    "    return SparseVector(len(OHEDict.keys()), index_list, values)\n",
    "\n",
    "def parseRDD(line, OHEDict):\n",
    "    \"\"\"emit label, sparseVector for the sample that has been already one hot encoded. \n",
    "    label = 1 or 0 \n",
    "    sparseVector = (total number of features, [list of indexes], [list of corresponding values])\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # split each value in the comma separated text\n",
    "    for index, feature in enumerate(line.split(',')[1:]):\n",
    "        idxfeat = index, feature\n",
    "        features.append(idxfeat)\n",
    "    \n",
    "    return LabeledPoint(line.split(\",\")[0], sparseOHE(features, OHEDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st sample of feature engineered train data\n",
      "[LabeledPoint(0.0, (234358,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,117215,117216,117217,117218,117219,117220,117221,117222,117223,117224,117225,117226,117227,117228,117229,117230,117231,117232,117233,117234,117235],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "--------------------------------------------------\n",
      "1st sample of feature engineered validation data\n",
      "[LabeledPoint(0.0, (234358,[2,8,15,21,45,49,50,52,61,96,135,160,164,671,1355,5140,5141,5142,117223,117226,117238,117242,117258,117261,117262,117267,117269,117284,117287,118378,119236,120375,122236,122237,122238,122239,122240,122241,170030],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "--------------------------------------------------\n",
      "1st sample of feature engineered test data\n",
      "[LabeledPoint(0.0, (234358,[8,15,33,45,49,50,53,55,81,107,328,374,375,573,1006,2489,3370,3374,3375,14074,21790,51370,112273,117223,117227,117234,117256,117261,117262,117265,117267,117287,117296,117380,120540,120542,120543,124243,124246],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "OHETrainData = TrainData.map(lambda line: parseRDD(line, trainOHEDict))\n",
    "OHEValData = ValData.map(lambda line: parseRDD(line, trainOHEDict))\n",
    "OHETestData = TestData.map(lambda line: parseRDD(line, trainOHEDict))\n",
    "print(\"1st sample of feature engineered train data\")\n",
    "print(OHETrainData.take(1))\n",
    "print(\"-\"*50)\n",
    "print(\"1st sample of feature engineered validation data\")\n",
    "print(OHEValData.take(1))\n",
    "print(\"-\"*50)\n",
    "print(\"1st sample of feature engineered test data\")\n",
    "print(OHETestData.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "## Logistic Regression on Criteo CTR Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights for the first 5 variables or features developed on train data\n",
      "\n",
      "[-0.34718045287758437, -0.345875988465329, -0.3324849806726338, -0.31288488747657667, -0.31038881937206875]\n",
      "--------------------------------------------------\n",
      "Model intercept\n",
      "\n",
      "0.596061956931456\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# adjustable hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 5.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "model0 = LogisticRegressionWithSGD.train(OHETrainData,\\\n",
    "                                         iterations=numIters,\\\n",
    "                                         step=stepSize,\\\n",
    "                                         regParam=regParam,\\\n",
    "                                         regType=regType,\\\n",
    "                                         intercept=includeIntercept)\n",
    "\n",
    "print(\"Model weights for the first 5 variables or features developed on train data\\n\")\n",
    "sortedWeights = sorted(model0.weights)\n",
    "print(sortedWeights[:5])\n",
    "print(\"-\"*50)\n",
    "print(\"Model intercept\\n\")\n",
    "print(model0.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp \n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "    Since log(0) is undefined, we will use a small value (epsilon). \n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "\n",
    "    if p == 1:\n",
    "        p = p - epsilon\n",
    "    elif p == 0:\n",
    "        p = p + epsilon\n",
    "    \n",
    "    if y == 1:\n",
    "        return -log(p)\n",
    "    elif y == 0: \n",
    "        return -log(1-p)\n",
    "\n",
    "def calcProb(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "    Note    : We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "    Returns : float: A probability between 0 and 1 for each sample using the model developed earlier, i.e., LogisticRegressionwithSGD\n",
    "    \"\"\"\n",
    "    \n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "\n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    \n",
    "    # pass it through the sigmoid function\n",
    "    return (1 + exp(-rawPrediction))**(-1)\n",
    "\n",
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\"\"\"\n",
    "    \n",
    "    # generate the predictions and the labels and combine them\n",
    "    predictions = data.map(lambda x: calcProb(x.features, model.weights, model.intercept))\n",
    "    labels = data.map(lambda x: x.label)\n",
    "    pred_labels = predictions.zip(labels)\n",
    "    \n",
    "    # compute the log-loss, sum it, and divide by the count\n",
    "    logLoss = (pred_labels.map(lambda x: computeLogLoss(x[0], x[1])) \n",
    "                .reduce(lambda x,y: x + y))/pred_labels.count()\n",
    "    \n",
    "    return logLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the baseline for logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22712452999887575\n",
      "Baseline Train Logloss = 0.536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the overall ctr\n",
    "classOneFracTrain = OHETrainData.map(lambda x: x.label).reduce(lambda x,y: x + y) / OHETrainData.count()\n",
    "print(classOneFracTrain)\n",
    "\n",
    "# using the overall ctr, applies to each sample and calculate the logloss\n",
    "logLossTrBase = (OHETrainData.map(lambda x: computeLogLoss(classOneFracTrain,x.label))\n",
    "                             .reduce(lambda x,y: x + y))/OHETrainData.count()\n",
    "\n",
    "print('Baseline Train Logloss = {0:.3f}\\n'.format(logLossTrBase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance (logloss) on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability prediction on first 5 samples using LogisticRegression with Stochastic Gradient Descent\n",
      "[0.28783712176420123, 0.12093271783135372, 0.3152123993814, 0.1700386154024066, 0.5676127642926942]\n",
      "--------------------------------------------------\n",
      "OHE Features Train Logloss:\n",
      "\tBaseline = 0.536\n",
      "\tLogistic Regression model = 0.467\n"
     ]
    }
   ],
   "source": [
    "trainingPredictions = OHETrainData.map(lambda x: calcProb(x.features,model0.weights,model0.intercept))\n",
    "\n",
    "print(\"Probability prediction on first 5 samples using LogisticRegression with Stochastic Gradient Descent\")\n",
    "print(trainingPredictions.take(5))\n",
    "\n",
    "print(\"-\"*50)\n",
    "logLossTrLR0 = evaluateResults(model0, OHETrainData)\n",
    "print('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'.format(logLossTrBase, logLossTrLR0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset to check the model performance\n",
    "\n",
    "Since we have developed out model on the train dataset, we will use the model on validation dataset and check the performance, i.e., logloss. If need be, we'd go back to the model, fine tune the hyperparameters to bring the logloss as near as zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE Features Validation Logloss:\n",
      "\tBaseline = 0.527\n",
      "\tLogistic Regression model = 0.465\n"
     ]
    }
   ],
   "source": [
    "logLossValBase = OHEValData.map(lambda x: computeLogLoss(classOneFracTrain,x.label))\\\n",
    ".reduce(lambda x,y: x + y) / OHEValData.count()\n",
    "\n",
    "logLossValLR0 = evaluateResults(model0,OHEValData)\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'\n",
    "       .format(logLossValBase, logLossValLR0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE Features Validation Logloss:\n",
      "\tBaseline = 0.539\n",
      "\tLogistic Regression model = 0.472\n"
     ]
    }
   ],
   "source": [
    "logLossTestBase = OHETestData.map(lambda x: computeLogLoss(classOneFracTrain,x.label))\\\n",
    ".reduce(lambda x,y: x + y) / OHETestData.count()\n",
    "\n",
    "logLossTestLR0 = evaluateResults(model0,OHETestData)\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'\n",
    "       .format(logLossTestBase, logLossTestLR0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion from Logistic Regression using One Hot Encoding  \n",
    "\n",
    "Our logistic regression with stochastic gradient descent performs slightly better than the baseline. We checked the baseline in such a way that the average click through rate is distributed as probability for each sample and the performance parameter (logloss) is calculated. As the best model that can predict accurately will have the log loss of `0` value, the performance of our model is checked with the logloss function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Features Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\"\"\"\n",
    "    \n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = (category + str(ind)).encode('utf-8')\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseHashPoint(line, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\"\"\"\n",
    "\n",
    "    # grab the label and feature \n",
    "    label = line.split(',')[0]\n",
    "    features = line.split(',')[1:]\n",
    "    \n",
    "    # create an array for the indexed features\n",
    "    indexedFeat = []\n",
    "    for i,feat in enumerate(features):\n",
    "        idxfeat = i,feat\n",
    "        indexedFeat.append(idxfeat)\n",
    "    \n",
    "    # convert the features to a sparse hash dictionary\n",
    "    hashed = hashFunction(numBuckets, indexedFeat)\n",
    "    sorted_hash = sorted([(v,k) for v,k in hashed.items()])\n",
    "    \n",
    "    # separate out the indices and the features\n",
    "    sorted_idx = [x[0] for x in sorted_hash]\n",
    "    sorted_feat = [x[1] for x in sorted_hash]\n",
    "    \n",
    "    # create the sparse vector\n",
    "    vector = SparseVector(numBuckets, sorted_idx, sorted_feat)\n",
    "    \n",
    "    # create the labelled point\n",
    "    new_point = LabeledPoint(label, vector)\n",
    "    \n",
    "    # return the labelled point\n",
    "    return new_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineered by Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st sample of feature hashed on train data\n",
      "[LabeledPoint(0.0, (32768,[1305,2883,3807,4814,4866,4913,6952,7117,9985,10316,11512,11722,12365,13893,14735,15816,16198,17761,19274,21604,22256,22563,22785,24855,25202,25533,25721,26487,26656,27668,28211,29152,29402,29873,30039,31484,32493,32708],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "--------------------------------------------------\n",
      "1st sample of feature hashed on validation data\n",
      "[LabeledPoint(0.0, (32768,[1580,2817,3338,3668,3807,4533,4667,5302,5725,7077,7998,8316,8759,8909,9114,11558,11722,12089,13606,13687,19274,20821,21734,22256,22580,22943,23554,23587,24234,25533,25818,25947,27939,28405,30065,30683,31035,31663],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n",
      "--------------------------------------------------\n",
      "1st sample of feature hashed on test data\n",
      "[LabeledPoint(0.0, (32768,[250,2069,2627,2817,5725,6357,7697,7762,9745,10188,11558,11722,12136,12629,12945,13026,13606,14296,14597,15048,17341,18011,18023,18689,18793,19274,20601,21604,22011,23039,23149,24234,25533,25818,26384,27095,28107,28211,28405],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "# generouly giving the hash table size = 32,768 slots\n",
    "numBuckets = 2 ** 15\n",
    "FHTrainData = TrainData.map(lambda x: parseHashPoint(x,numBuckets))\n",
    "FHValData = ValData.map(lambda x: parseHashPoint(x,numBuckets))\n",
    "FHTestData = TestData.map(lambda x: parseHashPoint(x,numBuckets))\n",
    "\n",
    "print(\"1st sample of feature hashed on train data\")\n",
    "print(FHTrainData.take(1))\n",
    "print(\"-\"*50)\n",
    "print(\"1st sample of feature hashed on validation data\")\n",
    "print(FHValData.take(1))\n",
    "print(\"-\"*50)\n",
    "print(\"1st sample of feature hashed on test data\")\n",
    "print(FHTestData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights for the first 5 variables or features developed on train data\n",
      "\n",
      "[-0.46290298227041093, -0.4016629631670746, -0.3823074413344563, -0.3628581823423947, -0.34856830469924743]\n",
      "--------------------------------------------------\n",
      "Model intercept\n",
      "\n",
      "0.5703368706362044\n"
     ]
    }
   ],
   "source": [
    "# Model definition on Feature hashed train dataset\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "# adjustable hyperparameters\n",
    "numIters = 50\n",
    "stepSize = 10.\n",
    "regParam = 1e-6\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "model1 = LogisticRegressionWithSGD.train(FHTrainData,\\\n",
    "                                         iterations=numIters,\\\n",
    "                                         step=stepSize,\\\n",
    "                                         regParam=regParam,\\\n",
    "                                         regType=regType,\\\n",
    "                                         intercept=includeIntercept)\n",
    "\n",
    "print(\"Model weights for the first 5 variables or features developed on train data\\n\")\n",
    "sortedWeights = sorted(model1.weights)\n",
    "print(sortedWeights[:5])\n",
    "print(\"-\"*50)\n",
    "print(\"Model intercept\\n\")\n",
    "print(model1.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability prediction on first 5 samples using LogisticRegression with Stochastic Gradient Descent\n",
      "[0.29951974875230575, 0.10025528557432511, 0.3040045161813861, 0.16816282602629815, 0.5388862232280243]\n",
      "--------------------------------------------------\n",
      "OHE Features Train Logloss:\n",
      "\tBaseline = 0.536\n",
      "\tLogistic Regression model = 0.458\n"
     ]
    }
   ],
   "source": [
    "trainingPredictions = FHTrainData.map(lambda x: calcProb(x.features,model1.weights,model1.intercept))\n",
    "\n",
    "print(\"Probability prediction on first 5 samples using LogisticRegression with Stochastic Gradient Descent\")\n",
    "print(trainingPredictions.take(5))\n",
    "\n",
    "print(\"-\"*50)\n",
    "logLossTrLR0 = evaluateResults(model1, FHTrainData)\n",
    "print('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'.format(logLossTrBase, logLossTrLR0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE Features Validation Logloss:\n",
      "\tBaseline = 0.527\n",
      "\tLogistic Regression model = 0.460\n"
     ]
    }
   ],
   "source": [
    "logLossValBase = FHValData.map(lambda x: computeLogLoss(classOneFracTrain, x.label))\\\n",
    ".reduce(lambda x,y: x + y) / FHValData.count()\n",
    "\n",
    "logLossValLR1 = evaluateResults(model1, FHValData)\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'\n",
    "       .format(logLossValBase, logLossValLR1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE Features Validation Logloss:\n",
      "\tBaseline = 0.539\n",
      "\tLogistic Regression model = 0.465\n"
     ]
    }
   ],
   "source": [
    "logLossTestBase = FHTestData.map(lambda x: computeLogLoss(classOneFracTrain, x.label))\\\n",
    ".reduce(lambda x,y: x + y) / FHTestData.count()\n",
    "\n",
    "logLossTestLR1 = evaluateResults(model1, FHTestData)\n",
    "print ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogistic Regression model = {1:.3f}'\n",
    "       .format(logLossTestBase, logLossTestLR1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "Our features engineering approach using hashing does not differ significantly from one hot encoding in terms of logloss value. The advantage is we significantly reduced our features space from `234,358` to `32,768`, accounting for 86% reduction in handling features alone. \n",
    "\n",
    "This is when we checked on `100,000` sample dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Application of Course Concepts\n",
    "Pick 3-5 key course concepts and discuss how your work on this assignment illustrates an understanding of these concepts.\n",
    "\n",
    "1. Parallel computation \n",
    "2. Dimensionality Reduction \n",
    "3. \n",
    "\n",
    "## 1. Parallel computation \n",
    "\n",
    "\n",
    "## 2. Dimensionality Reduction (PCA vs Features Hashing) \n",
    "\n",
    "Although both PCA and Features hashing generate reduction in features dimension as per se, the fundamental concept in dimensionality reduction is different in each approach. \n",
    "\n",
    "### PCA (Principal Component Analysis) \n",
    "\n",
    "In PCA, all features are considered. However, in calculating the eigenvectors, i.e., the vectors can be multiplied with a scalar value (eigenvalue) and the resulting vector stays in the same euclidean vector space without changing the directionality. For all the eigenvectors that can be made from the given features space, the highest eigenvector is calculated based on the how many number of features contributes to the vector and gives the highest variance. The higher the variance, the more likely it is that the model can capture all the dataset. Since each eigenvectors are orthogonal to each other, every vector has a variation in feature numbers. Some vector could have `10000` features contributing to the larger percentage of the variance, while others can be just `200` features. In principle, PCA offers the most significant features in the feature spaces. \n",
    "\n",
    "### Features Hashing\n",
    "\n",
    "In contrast to PCA, features hashing is constrained by the number of feature space we defined. We could generously give the feature space to be $2^{15} = 32,768$, i.e., there should be `32,768` features we want to analyze in our model development. We do not have a control on which features to choose and which features to drop. Granted, if we start off with a small feature space, it's more likely that we will be dropping many features via **hash collision** situation. \n",
    "\n",
    "It is also noteworthy that when the dataset is extremelty large, executing PCA on millions of features is computationaly expensive and features hashing is recommended.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
