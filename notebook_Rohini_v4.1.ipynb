{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__  \n",
    "Throughout this course you’ve engaged with key principles required to develop scalable machine learning analyses for structured and unstructured data. Working in Hadoop Streaming and Spark you’ve learned to translate common machine learning algorithms into Map-Reduce style implementations. You’ve developed the ability to evaluate Machine Learning approaches both in terms of their predictive performance as well as their scalability. For the final project you will demonstrate these skills by solving a machine learning challenge on a new dataset. Your job is to perform Click Through Rate prediction on a large dataset of Criteo advertising data made public as part of a Kaggle competition a few years back. As you perform your analysis, keep in mind that we are not grading you on the final performance of your model or how ‘advanced’ the techniques you use but rather on your ability to explain and develop a scalable machine learning approach to answering a real question.\n",
    "\n",
    "More about the dataset:\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -n 1000 data/train.txt > data/train1000.txt\n",
    "#!tail -n 200 data/train.txt > data/test200.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "#lines = sc.textFile(\"data/train.txt\")\n",
    "lines = sc.textFile(\"gs://w261-bucket-pav/notebooks/data/train.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "projectTrain = parts.map(lambda p: Row(label=int(p[0]), I1=p[1], I2=p[2],\\\n",
    "                    I3=p[3], I4=p[4], I5=p[5], I6=p[6],\\\n",
    "                    I7=p[7], I8=p[8], I9=p[9], I10=p[10],\\\n",
    "                    I11=p[11], I12=p[12], I13=p[13], C1=p[14], C2=p[15], C3=p[16],\\\n",
    "                    C4=p[17], C5=p[18], C6=p[19], C7=p[20], C8=p[21], C9=p[22],\\\n",
    "                    C10=p[23], C11=p[24], C12=p[25], C13=p[26], C14=p[27], C15=p[28],\\\n",
    "                    C16=p[29], C17=p[30], C18=p[31], C19=p[32], C20=p[33], C21=p[34],\\\n",
    "                    C22=p[35], C23=p[36], C24=p[37], C25=p[38], C26=p[39]))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "fullDF = sqlContext.createDataFrame(projectTrain)\n",
    "fullDF.registerTempTable(\"trainTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=fullDF.randomSplit([0.7, 0.1, 0.2],2018)\n",
    "trainDF,valDF,testDF = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+--------+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+-----+---+---+---+---+-----+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|C19|      C2|C20|     C21|     C22|     C23|     C24|C25|C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9| I1|I10|I11|I12|I13| I2| I3| I4|   I5| I6| I7| I8| I9|label|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+--------+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+-----+---+---+---+---+-----+\n",
      "|000d72dd|5bf6e891|81cae03e|e0d76380|d413ef3e|b28479f6|a66dcf27|1203a270|d4bb7bd8|7b49e3d2|   |d833535f|   |73d06dde|c9d4222a|32c7478e|aee52b6f|   |   |b00d1501|d16679b9|4cf72387|fe6b92e5|aa239c24|0b153874|a73ee510|   |   |  1|   |  3| 12|  1|  3|10020|111|  1| 36| 95|    0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+--------+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+-----+---+---+---+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                                    |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |(262144,[6050,41297,48035,79843,88321,90447,114826,138833,169796,176890,183700,186642,199053,207626,212354,222967,239110,252737],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0]) |\n",
      "|1    |(262144,[6050,41297,45494,79843,97116,104439,109941,114826,121032,121883,132134,141056,153709,158464,199053,204205,212354,219405],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[6050,47086,88321,88387,90447,94943,114826,126943,145561,150414,153709,171223,180510,199053,212354,216490,226055,228942],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0]) |\n",
      "|(262144,[6050,11843,54248,90447,94546,100218,114826,158786,172120,175981,183755,188731,195181,199053,210018,212354,227301,233086],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "LogisticRegression_403f8fa0de200cf0e187\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|C19|      C2|C20|     C21|C22|     C23|     C24|C25|C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9| I1|I10|I11|I12|I13| I2| I3| I4|  I5| I6| I7| I8| I9|label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "|013c8fe1|233e3a0c|a7b606c4|6aaba33c|eae197fd|b28479f6|2d0bb053|b041b04a|e5ba7672|2804effd|   |421b43cd|   |723b4dfd|   |3a171ecb|b34f3128|   |   |8162da11|29998ed1|25c83c98|fe6b92e5|7f9907fe|233428af|a73ee510|   |   |  1|   |   | -1|   |   |2849|382|  4| 17| 96|    0|(262144,[6050,470...|[5.94293869980976...|[0.99738256269719...|       0.0|\n",
      "|02f970ca|e6003298|e9561d8b|        |1cc9ac51|b28479f6|8eb53550|        |27c07bd6|626db04f|   |fb253005|   |        |   |32c7478e|        |   |   |        |        |384874ce|        |a90a99c5|5b392875|a73ee510|   |   |  4|   |  7|  9|  2|  7|6759|  7| 10|  7| 32|    1|(262144,[6050,118...|[-7.4169002612821...|[6.00648237584797...|       1.0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Model Intercept:  -1.3338716065286154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Feature Weight: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Feature Weight|\n",
      "+--------------------+\n",
      "|-0.05849690204951...|\n",
      "| -0.9296982796503528|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Evaluator:\n",
      "Done\n",
      "CPU times: user 4.61 s, sys: 128 ms, total: 4.74 s\n",
      "Wall time: 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Copied code from the cells below - Delete start from here\n",
    "\n",
    "trainhasher = FeatureHasher(inputCols=[\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\"],\n",
    "                       outputCol=\"features\")\n",
    "#trainDF.select(\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\").show(2)\n",
    "featurizedTrain = trainhasher.transform(trainDF)\n",
    "#featurizedTrain.show(3)\n",
    "featurizedTrain.select(\"label\",\"features\").show(2,truncate=False)\n",
    "\n",
    "\n",
    "#testhasher = FeatureHasher(inputCols=[\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\"],\n",
    "#                       outputCol=\"features\")\n",
    "#testDF.select(\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\").show(2)\n",
    "featurizedTest = trainhasher.transform(testDF)\n",
    "\n",
    "featurizedTest.select(\"features\").show(2,truncate=False)\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter = 10)\n",
    "lrModel = lr.fit(featurizedTrain)\n",
    "print(lrModel)\n",
    "\n",
    "predictions = lrModel.transform(featurizedTest)\n",
    "predictions.show(2)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)\n",
    "evaluator.getMetricName()\n",
    "print('Model Intercept: ', lrModel.intercept)\n",
    "weights = lrModel.coefficients\n",
    "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "display(weightsDF)\n",
    "weightsDF.show(2)\n",
    "print(\"Evaluator:\")\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)\n",
    "print(\"Done\")\n",
    "\n",
    "# Delete end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C1: string (nullable = true)\n",
      " |-- C10: string (nullable = true)\n",
      " |-- C11: string (nullable = true)\n",
      " |-- C12: string (nullable = true)\n",
      " |-- C13: string (nullable = true)\n",
      " |-- C14: string (nullable = true)\n",
      " |-- C15: string (nullable = true)\n",
      " |-- C16: string (nullable = true)\n",
      " |-- C17: string (nullable = true)\n",
      " |-- C18: string (nullable = true)\n",
      " |-- C19: string (nullable = true)\n",
      " |-- C2: string (nullable = true)\n",
      " |-- C20: string (nullable = true)\n",
      " |-- C21: string (nullable = true)\n",
      " |-- C22: string (nullable = true)\n",
      " |-- C23: string (nullable = true)\n",
      " |-- C24: string (nullable = true)\n",
      " |-- C25: string (nullable = true)\n",
      " |-- C26: string (nullable = true)\n",
      " |-- C3: string (nullable = true)\n",
      " |-- C4: string (nullable = true)\n",
      " |-- C5: string (nullable = true)\n",
      " |-- C6: string (nullable = true)\n",
      " |-- C7: string (nullable = true)\n",
      " |-- C8: string (nullable = true)\n",
      " |-- C9: string (nullable = true)\n",
      " |-- I1: string (nullable = true)\n",
      " |-- I10: string (nullable = true)\n",
      " |-- I11: string (nullable = true)\n",
      " |-- I12: string (nullable = true)\n",
      " |-- I13: string (nullable = true)\n",
      " |-- I2: string (nullable = true)\n",
      " |-- I3: string (nullable = true)\n",
      " |-- I4: string (nullable = true)\n",
      " |-- I5: string (nullable = true)\n",
      " |-- I6: string (nullable = true)\n",
      " |-- I7: string (nullable = true)\n",
      " |-- I8: string (nullable = true)\n",
      " |-- I9: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()\n",
    "trainDF.select('label').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}