{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__  \n",
    "Throughout this course you’ve engaged with key principles required to develop scalable machine learning analyses for structured and unstructured data. Working in Hadoop Streaming and Spark you’ve learned to translate common machine learning algorithms into Map-Reduce style implementations. You’ve developed the ability to evaluate Machine Learning approaches both in terms of their predictive performance as well as their scalability. For the final project you will demonstrate these skills by solving a machine learning challenge on a new dataset. Your job is to perform Click Through Rate prediction on a large dataset of Criteo advertising data made public as part of a Kaggle competition a few years back. As you perform your analysis, keep in mind that we are not grading you on the final performance of your model or how ‘advanced’ the techniques you use but rather on your ability to explain and develop a scalable machine learning approach to answering a real question.\n",
    "\n",
    "More about the dataset:\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1000 data/train.txt > data/train1000.txt\n",
    "!head -n 200 data/test.txt > data/test200.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"data/train1000.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "projectTrain = parts.map(lambda p: Row(label=int(p[0]), I1=p[1], I2=p[2],\\\n",
    "                    I3=p[3], I4=p[4], I5=p[5], I6=p[6],\\\n",
    "                    I7=p[7], I8=p[8], I9=p[9], I10=p[10],\\\n",
    "                    I11=p[11], I12=p[12], I13=p[13], C1=p[14], C2=p[15], C3=p[16],\\\n",
    "                    C4=p[17], C5=p[18], C6=p[19], C7=p[20], C8=p[21], C9=p[22],\\\n",
    "                    C10=p[23], C11=p[24], C12=p[25], C13=p[26], C14=p[27], C15=p[28],\\\n",
    "                    C16=p[29], C17=p[30], C18=p[31], C19=p[32], C20=p[33], C21=p[34],\\\n",
    "                    C22=p[35], C23=p[36], C24=p[37], C25=p[38], C26=p[39]))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "trainDF = sqlContext.createDataFrame(projectTrain)\n",
    "trainDF.registerTempTable(\"trainTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|     C19|      C2|     C20|     C21|     C22|     C23|     C24|     C25|     C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9| I1|I10|I11|I12|I13| I2| I3| I4|  I5| I6| I7| I8| I9|label|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+\n",
      "|68fd1e64|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|80e26c9b|b1252a9d|07b5194c|        |3a171ecb|c5c50484|e8b83407|9727dd16|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|  1|  1|  2|   |  2|  1|  5|  0|1382|  4| 15|  2|181|    0|\n",
      "|68fd1e64|2b53e5fb|4f1b46f3|623049e6|d7020589|b28479f6|e6c5b5cd|c92f3b61|07c540c4|b04e4670|21ddcdc9|f0cf0024|5840adea|60f6221e|        |3a171ecb|43f13e8b|e8b83407|731c3655|6f67f7e5|41274cd7|25c83c98|fe6b92e5|922afcc0|0b153874|a73ee510|  2|  1|  1|   |  4|  0| 44|  1| 102|  8|  2|  2|  4|    0|\n",
      "|287e684f|3b08e48b|5f5e6091|8fe001f4|aa655a2f|07d13a8f|6dc710ed|36103458|8efede7f|3412118d|        |0a519c5c|        |e587c466|ad3062eb|3a171ecb|3b183c5c|        |        |02cf9876|c18be181|25c83c98|7e0ccccf|c78204a1|0b153874|a73ee510|  2|  1|  3|  3| 45|  0|  1| 14| 767| 89|  4|  2|245|    0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C1: string (nullable = true)\n",
      " |-- C10: string (nullable = true)\n",
      " |-- C11: string (nullable = true)\n",
      " |-- C12: string (nullable = true)\n",
      " |-- C13: string (nullable = true)\n",
      " |-- C14: string (nullable = true)\n",
      " |-- C15: string (nullable = true)\n",
      " |-- C16: string (nullable = true)\n",
      " |-- C17: string (nullable = true)\n",
      " |-- C18: string (nullable = true)\n",
      " |-- C19: string (nullable = true)\n",
      " |-- C2: string (nullable = true)\n",
      " |-- C20: string (nullable = true)\n",
      " |-- C21: string (nullable = true)\n",
      " |-- C22: string (nullable = true)\n",
      " |-- C23: string (nullable = true)\n",
      " |-- C24: string (nullable = true)\n",
      " |-- C25: string (nullable = true)\n",
      " |-- C26: string (nullable = true)\n",
      " |-- C3: string (nullable = true)\n",
      " |-- C4: string (nullable = true)\n",
      " |-- C5: string (nullable = true)\n",
      " |-- C6: string (nullable = true)\n",
      " |-- C7: string (nullable = true)\n",
      " |-- C8: string (nullable = true)\n",
      " |-- C9: string (nullable = true)\n",
      " |-- I1: string (nullable = true)\n",
      " |-- I10: string (nullable = true)\n",
      " |-- I11: string (nullable = true)\n",
      " |-- I12: string (nullable = true)\n",
      " |-- I13: string (nullable = true)\n",
      " |-- I2: string (nullable = true)\n",
      " |-- I3: string (nullable = true)\n",
      " |-- I4: string (nullable = true)\n",
      " |-- I5: string (nullable = true)\n",
      " |-- I6: string (nullable = true)\n",
      " |-- I7: string (nullable = true)\n",
      " |-- I8: string (nullable = true)\n",
      " |-- I9: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()\n",
    "trainDF.select('label').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"data/test200.txt\")\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "projectTest = parts.map(lambda p: Row(I1=p[0], I2=p[1],\\\n",
    "                    I3=p[2], I4=p[3], I5=p[4], I6=p[5],\\\n",
    "                    I7=p[6], I8=p[7], I9=p[8], I10=p[9],\\\n",
    "                    I11=p[10], I12=p[11], I13=p[12], C1=p[13], C2=p[14], C3=p[15],\\\n",
    "                    C4=p[16], C5=p[17], C6=p[18], C7=p[19], C8=p[20], C9=p[21],\\\n",
    "                    C10=p[22], C11=p[23], C12=p[24], C13=p[25], C14=p[26], C15=p[27],\\\n",
    "                    C16=p[28], C17=p[29], C18=p[30], C19=p[31], C20=p[32], C21=p[33],\\\n",
    "                    C22=p[34], C23=p[35], C24=p[36], C25=p[37], C26=p[38]))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "testDF = sqlContext.createDataFrame(projectTest)\n",
    "testDF.registerTempTable(\"trainTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                                     |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |(262144,[16677,45494,58516,62755,85868,105839,114826,121731,136999,153709,158464,171091,186439,204452,212354,218268,231205,259623],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0])|\n",
      "|0    |(262144,[67625,68022,85868,88321,90447,101856,105425,114826,121731,153709,174084,176922,200921,207319,212354,226221,244352,259623],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainhasher = FeatureHasher(inputCols=[\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\"],\n",
    "                       outputCol=\"features\")\n",
    "#trainDF.select(\"label\",\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\").show(2)\n",
    "featurizedTrain = trainhasher.transform(trainDF)\n",
    "#featurizedTrain.show(3)\n",
    "featurizedTrain.select(\"label\",\"features\").show(2,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+\n",
      "| I1| I2| I3| I4|  I5| I6| I7| I8| I9|I10|I11|I12|      C1|      C2|      C3|     C14|      C5|\n",
      "+---+---+---+---+----+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+\n",
      "|   | 29| 50|  5|7260|437|  1|  4| 14|   |  1|  0|5a9ed9b0|a0e12995|a1e14474|1adce6ef|25c83c98|\n",
      "| 27| 17| 45| 28|   2| 28| 27| 29| 28|  1|  1|   |68fd1e64|960c983b|9fbfbfd5|f862f261|25c83c98|\n",
      "+---+---+---+---+----+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[6050,54797,55088,74464,84988,88321,153709,158464,158999,164086,165045,171645,173822,182165,199053,201297,222967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[8661,24291,66530,77557,85868,88321,105128,107058,114826,153709,161970,169408,174332,180503,188340,241323,259623],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testhasher = FeatureHasher(inputCols=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\"],\n",
    "                       outputCol=\"features\")\n",
    "testDF.select(\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"C1\", \"C2\", \"C3\", \"C14\",\"C5\").show(2)\n",
    "featurizedTest = testhasher.transform(testDF)\n",
    "\n",
    "featurizedTest.select(\"features\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_4ed5a3d8077d3935e50d\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter = 10)\n",
    "lrModel = lr.fit(featurizedTrain)\n",
    "print(lrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|     C19|      C2|     C20|     C21|C22|     C23|     C24|     C25|     C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9| I1|I10|I11|I12|I13| I2| I3| I4|  I5| I6| I7| I8| I9|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "|5a9ed9b0|de89c3d2|59cd5ae7|8d98db20|8b216f7b|1adce6ef|78c64a1d|3ecdadf7|3486227d|1616f155|21ddcdc9|a0e12995|5840adea|2c277e62|   |423fab69|54c91918|9b3e8820|e75c9ae9|a1e14474|08a40877|25c83c98|        |964d1fdd|5b392875|a73ee510|   |   |  1|  0|  6| 29| 50|  5|7260|437|  1|  4| 14|(262144,[6050,547...|[2.80842894099958...|[0.94312961228087...|       0.0|\n",
      "|68fd1e64|ca53fc84|67360210|895d8bbb|4f8e2224|f862f261|b4cc2435|4c0041e5|e5ba7672|b4abdd09|21ddcdc9|960c983b|5840adea|36a7ab86|   |32c7478e|85e4d73f|010f6491|ee63dd9b|9fbfbfd5|38c11726|25c83c98|7e0ccccf|fe06fd10|062b5529|a73ee510| 27|  1|  1|   | 23| 17| 45| 28|   2| 28| 27| 29| 28|(262144,[8661,242...|[3.04680419783314...|[0.95464435391594...|       0.0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(featurizedTest)\n",
    "predictions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C1: string (nullable = true)\n",
      " |-- C10: string (nullable = true)\n",
      " |-- C11: string (nullable = true)\n",
      " |-- C12: string (nullable = true)\n",
      " |-- C13: string (nullable = true)\n",
      " |-- C14: string (nullable = true)\n",
      " |-- C15: string (nullable = true)\n",
      " |-- C16: string (nullable = true)\n",
      " |-- C17: string (nullable = true)\n",
      " |-- C18: string (nullable = true)\n",
      " |-- C19: string (nullable = true)\n",
      " |-- C2: string (nullable = true)\n",
      " |-- C20: string (nullable = true)\n",
      " |-- C21: string (nullable = true)\n",
      " |-- C22: string (nullable = true)\n",
      " |-- C23: string (nullable = true)\n",
      " |-- C24: string (nullable = true)\n",
      " |-- C25: string (nullable = true)\n",
      " |-- C26: string (nullable = true)\n",
      " |-- C3: string (nullable = true)\n",
      " |-- C4: string (nullable = true)\n",
      " |-- C5: string (nullable = true)\n",
      " |-- C6: string (nullable = true)\n",
      " |-- C7: string (nullable = true)\n",
      " |-- C8: string (nullable = true)\n",
      " |-- C9: string (nullable = true)\n",
      " |-- I1: string (nullable = true)\n",
      " |-- I10: string (nullable = true)\n",
      " |-- I11: string (nullable = true)\n",
      " |-- I12: string (nullable = true)\n",
      " |-- I13: string (nullable = true)\n",
      " |-- I2: string (nullable = true)\n",
      " |-- I3: string (nullable = true)\n",
      " |-- I4: string (nullable = true)\n",
      " |-- I5: string (nullable = true)\n",
      " |-- I6: string (nullable = true)\n",
      " |-- I7: string (nullable = true)\n",
      " |-- I8: string (nullable = true)\n",
      " |-- I9: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       1.0|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()\n",
    "predictions.select(\"prediction\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|     C19|      C2|     C20|     C21|C22|     C23|     C24|     C25|     C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9| I1|I10|I11|I12|I13| I2| I3| I4|  I5| I6| I7| I8| I9|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "|5a9ed9b0|de89c3d2|59cd5ae7|8d98db20|8b216f7b|1adce6ef|78c64a1d|3ecdadf7|3486227d|1616f155|21ddcdc9|a0e12995|5840adea|2c277e62|   |423fab69|54c91918|9b3e8820|e75c9ae9|a1e14474|08a40877|25c83c98|        |964d1fdd|5b392875|a73ee510|   |   |  1|  0|  6| 29| 50|  5|7260|437|  1|  4| 14|(262144,[6050,547...|[2.80842894099958...|[0.94312961228087...|       0.0|\n",
      "|68fd1e64|ca53fc84|67360210|895d8bbb|4f8e2224|f862f261|b4cc2435|4c0041e5|e5ba7672|b4abdd09|21ddcdc9|960c983b|5840adea|36a7ab86|   |32c7478e|85e4d73f|010f6491|ee63dd9b|9fbfbfd5|38c11726|25c83c98|7e0ccccf|fe06fd10|062b5529|a73ee510| 27|  1|  1|   | 23| 17| 45| 28|   2| 28| 27| 29| 28|(262144,[8661,242...|[3.04680419783314...|[0.95464435391594...|       0.0|\n",
      "|09ca0b81|b04d3cfe|70dcd184|899eb56b|aca22cf9|b28479f6|a473257f|88f592e4|d4bb7bd8|bd17c3da|1d04f4a4|8947f767|a458ea53|82bdc0bb|   |32c7478e|5bdcd9c4|010f6491|cca57dcc|a87e61f7|c4ba2a67|25c83c98|7e0ccccf|ce6020cc|062b5529|a73ee510|  1|  1|  1|   |  2|  1| 19|  7|   1|  3|  1|  7|  7|(262144,[13428,45...|[6.23297273674095...|[0.99804024226371...|       0.0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+---+---+---+---+---+---+----+---+---+---+---+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"label\" does not exist.\\nAvailable fields: C1, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C2, C20, C21, C22, C23, C24, C25, C26, C3, C4, C5, C6, C7, C8, C9, I1, I10, I11, I12, I13, I2, I3, I4, I5, I6, I7, I8, I9, features, rawPrediction, probability, prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o282.evaluate.\n: java.lang.IllegalArgumentException: Field \"label\" does not exist.\nAvailable fields: C1, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C2, C20, C21, C22, C23, C24, C25, C26, C3, C4, C5, C6, C7, C8, C9, I1, I10, I11, I12, I13, I2, I3, I4, I5, I6, I7, I8, I9, features, rawPrediction, probability, prediction\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:266)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5bf6a0523995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(lr.explainParams())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMetricName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Intercept: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"label\" does not exist.\\nAvailable fields: C1, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C2, C20, C21, C22, C23, C24, C25, C26, C3, C4, C5, C6, C7, C8, C9, I1, I10, I11, I12, I13, I2, I3, I4, I5, I6, I7, I8, I9, features, rawPrediction, probability, prediction'"
     ]
    }
   ],
   "source": [
    "#print(lr.explainParams())\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)\n",
    "evaluator.getMetricName()\n",
    "print('Model Intercept: ', lrModel.intercept)\n",
    "weights = lrModel.coefficients\n",
    "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "display(weightsDF)\n",
    "weightsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
