{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__  \n",
    "Throughout this course you’ve engaged with key principles required to develop scalable machine learning analyses for structured and unstructured data. Working in Hadoop Streaming and Spark you’ve learned to translate common machine learning algorithms into Map-Reduce style implementations. You’ve developed the ability to evaluate Machine Learning approaches both in terms of their predictive performance as well as their scalability. For the final project you will demonstrate these skills by solving a machine learning challenge on a new dataset. Your job is to perform Click Through Rate prediction on a large dataset of Criteo advertising data made public as part of a Kaggle competition a few years back. As you perform your analysis, keep in mind that we are not grading you on the final performance of your model or how ‘advanced’ the techniques you use but rather on your ability to explain and develop a scalable machine learning approach to answering a real question.\n",
    "\n",
    "More about the dataset:\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1', 'C2', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C11', 'C13', 'C14', 'C15', 'C17', 'C18', 'C19', 'C20', 'C22', 'C23', 'C25', 'C26', 'label', 'I4', 'I13', 'I7', 'I11', 'I1']\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "#lines = sc.textFile(\"data/train50000.txt\")\n",
    "lines = sc.textFile(\"gs://midsw261/data/train.txt\")\n",
    "\n",
    "parts = lines.map(lambda l: l.split(\"\\t\"))\n",
    "projectTrain = parts.map(lambda p: Row(label=int(p[0]), I1=p[1], I2=p[2],\\\n",
    "                    I3=p[3], I4=p[4], I5=p[5], I6=p[6],\\\n",
    "                    I7=p[7], I8=p[8], I9=p[9], I10=p[10],\\\n",
    "                    I11=p[11], I12=p[12], I13=p[13], C1=p[14], C2=p[15], C3=p[16],\\\n",
    "                    C4=p[17], C5=p[18], C6=p[19], C7=p[20], C8=p[21], C9=p[22],\\\n",
    "                    C10=p[23], C11=p[24], C12=p[25], C13=p[26], C14=p[27], C15=p[28],\\\n",
    "                    C16=p[29], C17=p[30], C18=p[31], C19=p[32], C20=p[33], C21=p[34],\\\n",
    "                    C22=p[35], C23=p[36], C24=p[37], C25=p[38], C26=p[39]))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "fullDF = sqlContext.createDataFrame(projectTrain)\n",
    "fullDF.registerTempTable(\"trainTable\")\n",
    "fullDF= fullDF.withColumn(\"I1\", fullDF[\"I1\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I2\", fullDF[\"I2\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I3\", fullDF[\"I3\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I4\", fullDF[\"I4\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I5\", fullDF[\"I5\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I6\", fullDF[\"I6\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I8\", fullDF[\"I8\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I9\", fullDF[\"I9\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I10\", fullDF[\"I10\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I12\", fullDF[\"I12\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I13\", fullDF[\"I13\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I7\", fullDF[\"I7\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I11\", fullDF[\"I11\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"I1\", fullDF[\"I1\"].cast(IntegerType()))\n",
    "fullDF= fullDF.withColumn(\"label\", fullDF[\"label\"].cast(IntegerType()))\n",
    "\n",
    "\n",
    "#remove cat logs = C3, C4, C12, C16, C21 and C24 \n",
    "# include - C6,C9,C14,C17,C20,C22,C23\n",
    "# remove -  I3, I2, I8, I9\n",
    "# remove I15 - outliers\n",
    "# include - I4 & I13, I8 & I13, I7 & I11, I1 & I7\n",
    "# since #I13 appears in noth I4 and I8 - include - I4,I13,I7 & I11, I1 & I7\n",
    "\n",
    "catcols = ['C1','C2','C5','C6','C7','C8','C9','C10','C11','C11','C13','C14','C15','C17','C18','C19','C20','C22','C23','C25','C26']\n",
    "numcols = ['label','I4','I13','I7','I11', 'I1']\n",
    "allcols = catcols + numcols\n",
    "print(allcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|C19|      C2|C20|     C21|C22|     C23|     C24|C25|C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9|  I1| I10|I11| I12| I13| I2|  I3|  I4|  I5| I6| I7| I8| I9|label|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+\n",
      "|013c8fe1|233e3a0c|a7b606c4|6aaba33c|eae197fd|b28479f6|2d0bb053|b041b04a|e5ba7672|2804effd|   |421b43cd|   |723b4dfd|   |3a171ecb|b34f3128|   |   |8162da11|29998ed1|25c83c98|fe6b92e5|7f9907fe|233428af|a73ee510|null|null|  1|null|null| -1|null|null|2849|382|  4| 17| 96|    0|\n",
      "|02f970ca|e6003298|e9561d8b|        |1cc9ac51|b28479f6|8eb53550|        |27c07bd6|626db04f|   |fb253005|   |        |   |32c7478e|        |   |   |        |        |384874ce|        |a90a99c5|5b392875|a73ee510|null|null|  4|null|   7|  9|   2|   7|6759|  7| 10|  7| 32|    1|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- C1: string (nullable = true)\n",
      " |-- C10: string (nullable = true)\n",
      " |-- C11: string (nullable = true)\n",
      " |-- C12: string (nullable = true)\n",
      " |-- C13: string (nullable = true)\n",
      " |-- C14: string (nullable = true)\n",
      " |-- C15: string (nullable = true)\n",
      " |-- C16: string (nullable = true)\n",
      " |-- C17: string (nullable = true)\n",
      " |-- C18: string (nullable = true)\n",
      " |-- C19: string (nullable = true)\n",
      " |-- C2: string (nullable = true)\n",
      " |-- C20: string (nullable = true)\n",
      " |-- C21: string (nullable = true)\n",
      " |-- C22: string (nullable = true)\n",
      " |-- C23: string (nullable = true)\n",
      " |-- C24: string (nullable = true)\n",
      " |-- C25: string (nullable = true)\n",
      " |-- C26: string (nullable = true)\n",
      " |-- C3: string (nullable = true)\n",
      " |-- C4: string (nullable = true)\n",
      " |-- C5: string (nullable = true)\n",
      " |-- C6: string (nullable = true)\n",
      " |-- C7: string (nullable = true)\n",
      " |-- C8: string (nullable = true)\n",
      " |-- C9: string (nullable = true)\n",
      " |-- I1: integer (nullable = true)\n",
      " |-- I10: integer (nullable = true)\n",
      " |-- I11: integer (nullable = true)\n",
      " |-- I12: integer (nullable = true)\n",
      " |-- I13: integer (nullable = true)\n",
      " |-- I2: integer (nullable = true)\n",
      " |-- I3: integer (nullable = true)\n",
      " |-- I4: integer (nullable = true)\n",
      " |-- I5: integer (nullable = true)\n",
      " |-- I6: integer (nullable = true)\n",
      " |-- I7: integer (nullable = true)\n",
      " |-- I8: integer (nullable = true)\n",
      " |-- I9: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'I1', 'I10', 'I11', 'I12', 'I13', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'label']\n"
     ]
    }
   ],
   "source": [
    "splits=fullDF.randomSplit([0.7, 0.1, 0.2],2018)\n",
    "trainDF,valDF,testDF = splits\n",
    "testDF.show(2)\n",
    "testDF.printSchema()\n",
    "cols = testDF.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |(256,[6,34,35,36,54,55,74,79,81,86,95,115,119,130,140,175,183,185,199,210,213,225,231,250],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0])|\n",
      "|1    |(256,[0,32,36,37,38,46,54,56,57,61,68,81,83,86,95,109,115,130,153,173,175,183,199,210],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,3.0,4.0,3.0])   |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(256,[19,36,39,54,74,79,83,86,95,109,111,115,119,130,143,156,173,175,183,197,216,223],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,0.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0])            |\n",
      "|(256,[5,9,36,54,55,59,66,79,83,86,95,97,109,115,130,137,160,175,179,183,199,205,210,238],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,10.0,2.0,4.0,8.0,1.0,7.0,1.0])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "LogisticRegression_4668a30103640783b148\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "|      C1|     C10|     C11|     C12|     C13|     C14|     C15|     C16|     C17|     C18|C19|      C2|C20|     C21|C22|     C23|     C24|C25|C26|      C3|      C4|      C5|      C6|      C7|      C8|      C9|  I1| I10|I11| I12| I13| I2|  I3|  I4|  I5| I6| I7| I8| I9|label|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "|013c8fe1|233e3a0c|a7b606c4|6aaba33c|eae197fd|b28479f6|2d0bb053|b041b04a|e5ba7672|2804effd|   |421b43cd|   |723b4dfd|   |3a171ecb|b34f3128|   |   |8162da11|29998ed1|25c83c98|fe6b92e5|7f9907fe|233428af|a73ee510|null|null|  1|null|null| -1|null|null|2849|382|  4| 17| 96|    0|(256,[19,36,39,54...|[5.32157600650190...|[0.99513869890634...|       0.0|\n",
      "|02f970ca|e6003298|e9561d8b|        |1cc9ac51|b28479f6|8eb53550|        |27c07bd6|626db04f|   |fb253005|   |        |   |32c7478e|        |   |   |        |        |384874ce|        |a90a99c5|5b392875|a73ee510|null|null|  4|null|   7|  9|   2|   7|6759|  7| 10|  7| 32|    1|(256,[5,9,36,54,5...|[-3.8178960120863...|[0.02150151139583...|       1.0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+---+--------+---+--------+--------+---+---+--------+--------+--------+--------+--------+--------+--------+----+----+---+----+----+---+----+----+----+---+---+---+---+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Model Intercept:  -1.324654540511575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Feature Weight: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      Feature Weight|\n",
      "+--------------------+\n",
      "|-0.24794536238897322|\n",
      "| -0.1890799873097542|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Evaluator:\n",
      "0.9920416956594584\n",
      "Total  9170441\n",
      "Correct 8972315\n",
      "True Positive 6650937\n",
      "Wrong 198126\n",
      "False Negatibe 28609\n",
      "False Positive 169517\n",
      "Ratio Wrong 0.02160484975586234\n",
      "Ratio Correct 0.9783951502441377\n",
      "CPU times: user 484 ms, sys: 176 ms, total: 660 ms\n",
      "Wall time: 15min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Copied code from the cells below - Delete start from here\n",
    "\n",
    "num_of_features = 256 # 2^8\n",
    "\n",
    "hasher = FeatureHasher(inputCols=allcols,outputCol=\"features\",categoricalCols=catcols)\n",
    "hasher.setNumFeatures(num_of_features)\n",
    "\n",
    "featurizedTrain = hasher.transform(trainDF)\n",
    "featurizedTrain.select(\"label\",\"features\").show(2,truncate=False)\n",
    "\n",
    "featurizedTest = hasher.transform(testDF)\n",
    "\n",
    "featurizedTest.select(\"features\").show(2,truncate=False)\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",maxIter = 10)\n",
    "lrModel = lr.fit(featurizedTrain)\n",
    "print(lrModel)\n",
    "\n",
    "predictions = lrModel.transform(featurizedTest)\n",
    "predictions.show(2)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)\n",
    "evaluator.getMetricName()\n",
    "print('Model Intercept: ', lrModel.intercept)\n",
    "weights = lrModel.coefficients\n",
    "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "display(weightsDF)\n",
    "weightsDF.show(2)\n",
    "print(\"Evaluator:\")\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
    "eval = evaluator.evaluate(predictions)\n",
    "print(eval)\n",
    "\n",
    "#prediction analysis\n",
    "lp = predictions.select( \"label\", \"prediction\")\n",
    "counttotal = predictions.count()\n",
    "correct = lp.filter(lp.label == lp.prediction).count()\n",
    "print(\"Total \",counttotal)\n",
    "print(\"Correct\",correct)\n",
    "wrong = lp.filter(lp.label != lp.prediction).count()\n",
    "truep = lp.filter(lp.prediction == 0.0).filter(lp.label == lp.prediction).count()\n",
    "print(\"True Positive\",truep)\n",
    "print(\"Wrong\",wrong)\n",
    "falseN = lp.filter(lp.prediction == 0.0).filter(lp.label != lp.prediction).count()\n",
    "falseP = lp.filter(lp.prediction == 1.0).filter(lp.label != lp.prediction).count()\n",
    "\n",
    "print(\"False Negatibe\",falseN)\n",
    "print(\"False Positive\",falseP) \n",
    "ratioWrong = wrong/float(counttotal)\n",
    "print(\"Ratio Wrong\",ratioWrong)\n",
    "rationCorrect = correct/float(counttotal)\n",
    "print(\"Ratio Correct\",rationCorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
