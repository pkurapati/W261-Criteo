{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__  \n",
    "Throughout this course you’ve engaged with key principles required to develop scalable machine learning analyses for structured and unstructured data. Working in Hadoop Streaming and Spark you’ve learned to translate common machine learning algorithms into Map-Reduce style implementations. You’ve developed the ability to evaluate Machine Learning approaches both in terms of their predictive performance as well as their scalability. For the final project you will demonstrate these skills by solving a machine learning challenge on a new dataset. Your job is to perform Click Through Rate prediction on a large dataset of Criteo advertising data made public as part of a Kaggle competition a few years back. As you perform your analysis, keep in mind that we are not grading you on the final performance of your model or how ‘advanced’ the techniques you use but rather on your ability to explain and develop a scalable machine learning approach to answering a real question.\n",
    "\n",
    "More about the dataset:\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data directory (RUN THIS CELL AS IS)\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Old option `g' requires an argument.\n",
      "Try `tar --help' or `tar --usage' for more information.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4364M  100 4364M    0     0  9505k      0  0:07:50  0:07:50 --:--:-- 10.7M\n"
     ]
    }
   ],
   "source": [
    "# grab the tar.gz file from kaggle\n",
    "!curl https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz -o data/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# I couldn't get this to unpack the tarball, so I just did it in Windows\n",
    "#!tar -xvz data/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t5\t0\t1382\t4\t15\t2\t181\t1\t2\t\t2\t68fd1e64\t80e26c9b\tfb936136\t7b4723c4\t25c83c98\t7e0ccccf\tde7995b8\t1f89b562\ta73ee510\ta8cd5504\tb2cb9c98\t37c9c164\t2824a5f6\t1adce6ef\t8ba8b39a\t891b62e7\te5ba7672\tf54016b9\t21ddcdc9\tb1252a9d\t07b5194c\t\t3a171ecb\tc5c50484\te8b83407\t9727dd16\n",
      "0\t2\t0\t44\t1\t102\t8\t2\t2\t4\t1\t1\t\t4\t68fd1e64\tf0cf0024\t6f67f7e5\t41274cd7\t25c83c98\tfe6b92e5\t922afcc0\t0b153874\ta73ee510\t2b53e5fb\t4f1b46f3\t623049e6\td7020589\tb28479f6\te6c5b5cd\tc92f3b61\t07c540c4\tb04e4670\t21ddcdc9\t5840adea\t60f6221e\t\t3a171ecb\t43f13e8b\te8b83407\t731c3655\n",
      "0\t2\t0\t1\t14\t767\t89\t4\t2\t245\t1\t3\t3\t45\t287e684f\t0a519c5c\t02cf9876\tc18be181\t25c83c98\t7e0ccccf\tc78204a1\t0b153874\ta73ee510\t3b08e48b\t5f5e6091\t8fe001f4\taa655a2f\t07d13a8f\t6dc710ed\t36103458\t8efede7f\t3412118d\t\t\te587c466\tad3062eb\t3a171ecb\t3b183c5c\t\t\n",
      "0\t\t893\t\t\t4392\t\t0\t0\t0\t\t0\t\t\t68fd1e64\t2c16a946\ta9a87e68\t2e17d6f6\t25c83c98\tfe6b92e5\t2e8a689b\t0b153874\ta73ee510\tefea433b\te51ddf94\ta30567ca\t3516f6e6\t07d13a8f\t18231224\t52b8680f\t1e88c74f\t74ef3502\t\t\t6b3a5ca6\t\t3a171ecb\t9117a34a\t\t\n",
      "0\t3\t-1\t\t0\t2\t0\t3\t0\t0\t1\t1\t\t0\t8cf07265\tae46a29d\tc81688bb\tf922efad\t25c83c98\t13718bbd\tad9fa255\t0b153874\ta73ee510\t5282c137\te5d8af57\t66a76a26\tf06c53ac\t1adce6ef\t8ff4b403\t01adbab4\t1e88c74f\t26b3c7a7\t\t\t21c9516a\t\t32c7478e\tb34f3128\t\t\n",
      "0\t\t-1\t\t\t12824\t\t0\t0\t6\t\t0\t\t\t05db9164\t6c9c9cf3\t2730ec9c\t5400db8b\t43b19349\t6f6d9be8\t53b5f978\t0b153874\ta73ee510\t3b08e48b\t91e8fc27\tbe45b877\t9ff13f22\t07d13a8f\t06969a20\t9bc7fff5\t776ce399\t92555263\t\t\t242bb710\t8ec974f4\tbe7c41b4\t72c78f11\t\t\n",
      "0\t\t1\t2\t\t3168\t\t0\t1\t2\t\t0\t\t\t439a44a4\tad4527a2\tc02372d0\td34ebbaa\t43b19349\tfe6b92e5\t4bc6ffea\t0b153874\ta73ee510\t3b08e48b\ta4609aab\t14d63538\t772a00d7\t07d13a8f\tf9d1382e\tb00d3dc9\t776ce399\tcdfa8259\t\t\t20062612\t\t93bad2c0\t1b256e61\t\t\n",
      "1\t1\t4\t2\t0\t0\t0\t1\t0\t0\t1\t1\t\t0\t68fd1e64\t2c16a946\t503b9dbc\te4dbea90\tf3474129\t13718bbd\t38eb9cf4\t1f89b562\ta73ee510\t547c0ffe\tbc8c9f21\t60ab2f07\t46f42a63\t07d13a8f\t18231224\te6b6bdc7\te5ba7672\t74ef3502\t\t\t5316a17f\t\t32c7478e\t9117a34a\t\t\n",
      "0\t\t44\t4\t8\t19010\t249\t28\t31\t141\t\t1\t\t8\t05db9164\td833535f\td032c263\tc18be181\t25c83c98\t7e0ccccf\td5b6acf2\t0b153874\ta73ee510\t2acdcf4e\t086ac2d2\tdfbb09fb\t41a6ae00\tb28479f6\te2502ec9\t84898b2a\te5ba7672\t42a2edb9\t\t\t0014c32a\t\t32c7478e\t3b183c5c\t\t\n",
      "0\t\t35\t\t1\t33737\t21\t1\t2\t3\t\t1\t\t1\t05db9164\t510b40a5\td03e7c24\teb1fd928\t25c83c98\t\t52283d1c\t0b153874\ta73ee510\t015ac893\te51ddf94\t951fe4a9\t3516f6e6\t07d13a8f\t2ae4121c\t8ec71479\td4bb7bd8\t70d0f5f9\t\t\t0e63fca0\t\t32c7478e\t0e8fe315\t\t\n"
     ]
    }
   ],
   "source": [
    "# take a look at the to the training data set\n",
    "!head data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into Spark RDD for convenience of use later (RUN THIS CELL AS IS)\n",
    "projectRDD = sc.textFile('data/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Question Formulation \n",
    "Introduce the goal of your analysis. What questions will you seek to answer, why do people perform this kind of analysis on this kind of data? Preview what level of performance your model would need to achieve to be practically useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Algorithm Explanation\n",
    "Create your own toy example that matches the dataset provided and use this toy example to explain the math behind the algorithym that you will perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: EDA & Discussion of Challenges\n",
    "Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable. Discuss any challenges that you anticipate based on the EDA you perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_CTR = projectRDD.map(lambda x: int(x.split('\\t')[0])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25622338372976045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_CTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Algorithm Implementation \n",
    "Develop a 'homegrown' implementation of the algorithn, apply it to the training dataset and evaluate your results on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Application of Course Concepts\n",
    "Pick 3-5 key course concepts and discuss how your work on this assignment illustrates an understanding of these concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
