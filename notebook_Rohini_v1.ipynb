{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__  \n",
    "Throughout this course you’ve engaged with key principles required to develop scalable machine learning analyses for structured and unstructured data. Working in Hadoop Streaming and Spark you’ve learned to translate common machine learning algorithms into Map-Reduce style implementations. You’ve developed the ability to evaluate Machine Learning approaches both in terms of their predictive performance as well as their scalability. For the final project you will demonstrate these skills by solving a machine learning challenge on a new dataset. Your job is to perform Click Through Rate prediction on a large dataset of Criteo advertising data made public as part of a Kaggle competition a few years back. As you perform your analysis, keep in mind that we are not grading you on the final performance of your model or how ‘advanced’ the techniques you use but rather on your ability to explain and develop a scalable machine learning approach to answering a real question.\n",
    "\n",
    "More about the dataset:\n",
    "https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory `data': File exists\n"
     ]
    }
   ],
   "source": [
    "# create a data directory (RUN THIS CELL AS IS)\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the tar.gz file from kaggle\n",
    "#!curl https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz -o data/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldn't get this to unpack the tarball, so I just did it in Windows\n",
    "#!tar -xvz data/dac.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the to the training data set\n",
    "!head -n 2000 data/train > data/train200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10015211672544614902,0,14102100,1005,1,e151e245,7e091613,f028772b,ecad2386,7801e8d9,07d7df22,a99f214a,42606fe6,cb0fb677,1,0,17037,320,50,1934,2,39,-1,16\n",
      "10015376300289320595,0,14102100,1005,0,1fbe01fe,f3845767,28905ebd,ecad2386,7801e8d9,07d7df22,a99f214a,03108db9,a0f5f879,1,0,15701,320,50,1722,0,35,100084,79\n",
      "10015405794859644629,1,14102100,1005,0,1fbe01fe,f3845767,28905ebd,ecad2386,7801e8d9,07d7df22,a99f214a,0b697be1,1f0bc64f,1,0,15701,320,50,1722,0,35,100084,79\n",
      "10015629448289660116,1,14102100,1005,0,1fbe01fe,f3845767,28905ebd,ecad2386,7801e8d9,07d7df22,a99f214a,58db4f0c,6332421a,1,0,15708,320,50,1722,0,35,-1,79\n",
      "100156980486870304,0,14102100,1005,0,1fbe01fe,f3845767,28905ebd,ecad2386,7801e8d9,07d7df22,a99f214a,02b9b0fc,1aa0e912,1,0,15706,320,50,1722,0,35,-1,79\n"
     ]
    }
   ],
   "source": [
    "!head -100 data/train | tail -n 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40428968"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data into Spark RDD for convenience of use later (RUN THIS CELL AS IS)\n",
    "projectRDD = sc.textFile('data/train')\n",
    "projectRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all categorical variables\n",
    "def get_categorical(df):\n",
    "    categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "    return categorical\n",
    "#get all numerical variables\n",
    "def get_numerical(df):\n",
    "    numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "    \n",
    "    return numerical\n",
    "\n",
    "numeric_features = get_numerical(train_df)\n",
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "parse_date = lambda val :pd.datetime.strptime(val,'%y%m%d%H')\n",
    "train_df = pd.read_csv('data/train2000',parse_dates=['hour'],date_parser=parse_date)\n",
    "corrmat = train_df.corr()\n",
    "k = 25\n",
    "cols = corrmat.nlargest(k,'click')['click'].index\n",
    "print(cols)\n",
    "cm = np.corrcoef(train_df[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "plt.figure(figsize = (20,10))\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n",
    "                 yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as n\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['click'].value_counts()/len(train)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hour.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.C1.value_counts()/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import pyspark\n",
    "#import urllib2\n",
    "\n",
    "#response = urllib2.urlopen('https://www.dropbox.com/s/rf64jk6eufmserm/dac_sample.txt?dl=1')\n",
    "\n",
    "#dacContents = response.read().split('\\n')\n",
    "file = open('data/train2000', 'r') \n",
    "dacContents = file.read().split(\"\\n\") \n",
    "dacContents = [x.strip().replace('\\t', ',') for x in dacContents]\n",
    "\n",
    "numPartitions = 2\n",
    "rawData = sc.parallelize(dacContents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights,seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = rawTrainData.count()\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print( nTrain, nVal, nTest, nTrain + nVal + nTest)\n",
    "print (rawData.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_features_all = point.split(\",\")\n",
    "    list_final = []\n",
    "    n = len(list_features_all)\n",
    "    for i in range(1,n):\n",
    "      list_final.append((i-1,list_features_all[i]))\n",
    "    \n",
    "    return list_final\n",
    "\n",
    "parsedTrainFeat = rawTrainData.map(parsePoint)\n",
    "\n",
    "numCategories = (parsedTrainFeat\n",
    "                 .flatMap(lambda x: x)\n",
    "                 .distinct()\n",
    "                 .map(lambda x: (x[0], 1))\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "                 .sortByKey()\n",
    "                 .collect())\n",
    "print(numCategories)\n",
    "\n",
    "print (numCategories[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for ind, category in rawFeats:\n",
    "        featureString = category + str(ind)\n",
    "        #signature = hashlib.sha256(h.encode('utf-8')).hexdigest()\n",
    "        #mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString.encode('utf-8')).hexdigest(), 16) % numBuckets)\n",
    "        \n",
    "    if(printMapping): print (mapping)\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parseHashPoint(point, numBuckets):\n",
    "    \"\"\"Create a LabeledPoint for this observation using hashing.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest are\n",
    "            features.\n",
    "        numBuckets: The number of buckets to hash to.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n",
    "            features.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_features_all = point.split(\",\")\n",
    "    list_final = []\n",
    "    n = len(list_features_all)\n",
    "    for i in range(1,n):\n",
    "      list_final.append((i-1,list_features_all[i]))\n",
    "   \n",
    "   \n",
    "    sp = SparseVector(numBuckets,hashFunction(numBuckets, list_final, True))\n",
    "    \n",
    "    if (list_features_all[0] == \"0\"):\n",
    "      lbp = LabeledPoint(0,sp)\n",
    "    else:\n",
    "      lbp = LabeledPoint(1,sp)\n",
    "  \n",
    "    return lbp\n",
    "\n",
    "numBucketsCTR = 2 ** 15\n",
    "hashTrainData = rawTrainData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "hashTrainData.cache()\n",
    "hashValidationData = rawValidationData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "hashValidationData.cache()\n",
    "hashTestData = rawTestData.map(lambda x: parseHashPoint(x, numBucketsCTR))\n",
    "hashTestData.cache()\n",
    "\n",
    "#signature = hashlib.sha256(h.encode('utf-8')).hexdigest()\n",
    "print( hashTrainData.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateResults(model, data):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Args:\n",
    "        model (LogisticRegressionModel): A trained logistic regression model.\n",
    "        data (RDD of LabeledPoint): Labels and features for each observation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.map(lambda x: computeLogLoss(getP(x.features, model.weights, model.intercept),x.label)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    \n",
    "    if p==0:\n",
    "      p+=epsilon\n",
    "    elif p==1:\n",
    "      p-=epsilon\n",
    "    \n",
    "    if y==1:\n",
    "      logloss = -log(p)\n",
    "    elif y==0:\n",
    "      logloss = -log(1-p)\n",
    "    \n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def getP(x, w, intercept):\n",
    "    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n",
    "\n",
    "    Note:\n",
    "        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "    Args:\n",
    "        x (SparseVector): A vector with values of 1.0 for features that exist in this\n",
    "            observation and 0.0 otherwise.\n",
    "        w (DenseVector): A vector of weights (betas) for the model.\n",
    "        intercept (float): The model's intercept.\n",
    "\n",
    "    Returns:\n",
    "        float: A probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    rawPrediction = x.dot(w) + intercept\n",
    "    \n",
    "    # Bound the raw prediction value\n",
    "    rawPrediction = min(rawPrediction, 20)\n",
    "    rawPrediction = max(rawPrediction, -20)\n",
    "    return 1/(1+exp(-rawPrediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "numIters = 500\n",
    "regType = 'l2'\n",
    "includeIntercept = True\n",
    "\n",
    "# Initialize variables using values from initial model training\n",
    "bestModel = None\n",
    "bestLogLoss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepSizes = [1,10]\n",
    "regParams = [1e-6,1e-3]\n",
    "for stepSize in stepSizes:\n",
    "    print(\"#stepSize\",stepSize)\n",
    "    for regParam in regParams:\n",
    "        model = (LogisticRegressionWithSGD\n",
    "                 .train(hashTrainData, numIters, stepSize, regParam=regParam, regType=regType,\n",
    "                        intercept=includeIntercept))\n",
    "        logLossVa = evaluateResults(model, hashValidationData)\n",
    "        print ('\\tstepSize = {0:.1f}, regParam = {1:.0e}: logloss = {2:.3f}'\n",
    "               .format(stepSize, regParam, logLossVa))\n",
    "        if (logLossVa < bestLogLoss):\n",
    "            bestModel = model\n",
    "            bestLogLoss = logLossVa\n",
    "\n",
    "print(type(bestLogLoss))\n",
    "print(bestLogLoss)\n",
    "#print ('Hashed Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "print ('Hashed Features Validation Logloss:\\tLogReg = {}'\n",
    "       .format(bestLogLoss))\n",
    "#       .format(logLossValBase, bestLogLoss))\n",
    "#stepSize = 1.0, regParam = 1e-06: logloss = 0.470\n",
    "#stepSize = 1.0, regParam = 1e-03: logloss = 0.470\n",
    "#stepSize = 10.0, regParam = 1e-06: logloss = 0.446\n",
    "#stepSize = 10.0, regParam = 1e-03: logloss = 0.448\n",
    "#Hashed Features Validation Logloss:\n",
    "#Baseline = 0.523\n",
    "#LogReg = 0.446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- click: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- C1: string (nullable = true)\n",
      " |-- banner_pos: string (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- device_conn_type: string (nullable = true)\n",
      " |-- C14: string (nullable = true)\n",
      " |-- C15: string (nullable = true)\n",
      " |-- C16: string (nullable = true)\n",
      " |-- C17: string (nullable = true)\n",
      " |-- C18: string (nullable = true)\n",
      " |-- C19: string (nullable = true)\n",
      " |-- C20: string (nullable = true)\n",
      " |-- C21: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "| 1000009418151094273|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| ddd2926e|    44956a24|          1|               2|15706|320| 50|1722|  0| 35|    -1| 79|\n",
      "|10000169349117863715|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 96809ac8|    711ee120|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+--------------------+\n",
      "|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|            features|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+--------------------+\n",
      "| 1000009418151094273|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| ddd2926e|    44956a24|          1|               2|15706|320| 50|1722|  0| 35|    -1| 79|(262144,[58495,60...|\n",
      "|10000169349117863715|    0|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 96809ac8|    711ee120|          1|               0|15704|320| 50|1722|  0| 35|100084| 79|(262144,[58495,60...|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|features                                                                              |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|(262144,[58495,60649,70572,132157,185204,196392,261742],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[58495,60649,70572,116820,132157,185204,196392],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[58495,60649,70572,132157,148881,185204,196392],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[52360,58495,60649,70572,132157,185204,196392],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|(262144,[60649,62986,70572,150764,194713,196392,207917],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|click|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(262144,[58495,60...|\n",
      "|    0|(262144,[58495,60...|\n",
      "|    0|(262144,[58495,60...|\n",
      "|    0|(262144,[52360,58...|\n",
      "|    0|(262144,[60649,62...|\n",
      "|    0|(262144,[58495,60...|\n",
      "|    0|(262144,[58495,60...|\n",
      "|    0|(262144,[9681,212...|\n",
      "|    1|(262144,[58495,60...|\n",
      "|    0|(262144,[58495,66...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "lrmodel LogisticRegression_4955ad058eb4d6367684\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|    hour|  C1|banner_pos| site_id|site_domain|click|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|9998354075836702668|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998487258543214200|14103023|1005|         0|83a0ad1a|   5c9ae867|    1|(262144,[19161,58...|[2.80035031060445...|[0.94269475129046...|       0.0|\n",
      "|9998515968748286661|14103023|1005|         1|856e6d3f|   58a89a43|    0|(262144,[23339,60...|[3.61657629684801...|[0.97382880887425...|       0.0|\n",
      "|9998613662398752368|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998654904628431953|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998752756639797808|14103023|1005|         1|e151e245|   7e091613|    1|(262144,[21201,60...|[2.19899062817109...|[0.90015883233146...|       0.0|\n",
      "|9999037534674210613|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9999585120349625051|14103023|1005|         1|f61eaaae|   6b59f079|    0|(262144,[60649,62...|[3.60380023112514...|[0.97350121606086...|       0.0|\n",
      "|9999636335882369227|14103023|1005|         0|85f751fd|   c4e18dd6|    1|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9999746639881208566|14103023|1005|         0|1fbe01fe|   f3845767|    0|(262144,[58495,60...|[2.35567656882702...|[0.91338437509360...|       0.0|\n",
      "|9807335364225058375|14103023|1005|         1|d9750ee7|   98572c79|    0|(262144,[57937,60...|[1.11618944793905...|[0.75328121427786...|       0.0|\n",
      "|9807357119418265272|14103023|1005|         0|85f751fd|   c4e18dd6|    1|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9807381820629414034|14103023|1005|         0|1fbe01fe|   f3845767|    0|(262144,[58495,60...|[2.35567656882702...|[0.91338437509360...|       0.0|\n",
      "|9807542303422388832|14103023|1005|         1|d9750ee7|   98572c79|    0|(262144,[57937,60...|[1.11618944793905...|[0.75328121427786...|       0.0|\n",
      "|9807579838502280834|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9807627606338478431|14103023|1005|         1|d9750ee7|   98572c79|    1|(262144,[57937,60...|[1.11618944793905...|[0.75328121427786...|       0.0|\n",
      "|9807633646626546770|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9807838192662182288|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9807891578712710933|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9808095871817015482|14103023|1005|         0|cd58172f|   b9c4ab81|    0|(262144,[58495,60...|[3.70742752667593...|[0.97604724227000...|       0.0|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+\n",
      "|                 id|    hour|  C1|banner_pos| site_id|site_domain|click|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+\n",
      "|9998354075836702668|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9998487258543214200|14103023|1005|         0|83a0ad1a|   5c9ae867|    1|\n",
      "|9998515968748286661|14103023|1005|         1|856e6d3f|   58a89a43|    0|\n",
      "|9998613662398752368|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9998654904628431953|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9998752756639797808|14103023|1005|         1|e151e245|   7e091613|    1|\n",
      "|9999037534674210613|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9999585120349625051|14103023|1005|         1|f61eaaae|   6b59f079|    0|\n",
      "|9999636335882369227|14103023|1005|         0|85f751fd|   c4e18dd6|    1|\n",
      "|9999746639881208566|14103023|1005|         0|1fbe01fe|   f3845767|    0|\n",
      "|9807335364225058375|14103023|1005|         1|d9750ee7|   98572c79|    0|\n",
      "|9807357119418265272|14103023|1005|         0|85f751fd|   c4e18dd6|    1|\n",
      "|9807381820629414034|14103023|1005|         0|1fbe01fe|   f3845767|    0|\n",
      "|9807542303422388832|14103023|1005|         1|d9750ee7|   98572c79|    0|\n",
      "|9807579838502280834|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9807627606338478431|14103023|1005|         1|d9750ee7|   98572c79|    1|\n",
      "|9807633646626546770|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9807838192662182288|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9807891578712710933|14103023|1005|         0|85f751fd|   c4e18dd6|    0|\n",
      "|9808095871817015482|14103023|1005|         0|cd58172f|   b9c4ab81|    0|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|                 id|    hour|  C1|banner_pos| site_id|site_domain|click|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|9998354075836702668|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998487258543214200|14103023|1005|         0|83a0ad1a|   5c9ae867|    1|(262144,[19161,58...|[2.80035031060445...|[0.94269475129046...|       0.0|\n",
      "|9998515968748286661|14103023|1005|         1|856e6d3f|   58a89a43|    0|(262144,[23339,60...|[3.61657629684801...|[0.97382880887425...|       0.0|\n",
      "|9998613662398752368|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998654904628431953|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9998752756639797808|14103023|1005|         1|e151e245|   7e091613|    1|(262144,[21201,60...|[2.19899062817109...|[0.90015883233146...|       0.0|\n",
      "|9999037534674210613|14103023|1005|         0|85f751fd|   c4e18dd6|    0|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9999585120349625051|14103023|1005|         1|f61eaaae|   6b59f079|    0|(262144,[60649,62...|[3.60380023112514...|[0.97350121606086...|       0.0|\n",
      "|9999636335882369227|14103023|1005|         0|85f751fd|   c4e18dd6|    1|(262144,[58495,60...|[2.92757974805002...|[0.94919308384090...|       0.0|\n",
      "|9999746639881208566|14103023|1005|         0|1fbe01fe|   f3845767|    0|(262144,[58495,60...|[2.35567656882702...|[0.91338437509360...|       0.0|\n",
      "+-------------------+--------+----+----------+--------+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "regParams = [1e-6,1e-3]\n",
    "sqlContext = SQLContext(sc)\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").load(\"data/train2000\")\n",
    "df.printSchema()\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"id\", \"click\", \"hour\", \"C1\",\"banner_pos\",\"site_id\",\"site_domain\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "testDF = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").load(\"data/train_tail_10\")\n",
    "testDF = testDF.select(\"id\", \"hour\", \"C1\",\"banner_pos\",\"site_id\",\"site_domain\",\"click\")\n",
    "\n",
    "testDF= testDF.withColumn(\"click\", testDF[\"click\"].cast(IntegerType()))\n",
    "\n",
    "hashertest = FeatureHasher(inputCols=[\"id\", \"hour\", \"click\",\"C1\",\"banner_pos\",\"site_id\",\"site_domain\"],\n",
    "                       outputCol=\"features\")\n",
    "ftestDF = hashertest.transform(testDF)\n",
    "\n",
    "\n",
    "featurized = hasher.transform(df)\n",
    "df.show(2)\n",
    "featurized.show(2)\n",
    "featurized.select('features').show(5,truncate=False)\n",
    "\n",
    "\n",
    "numIters = 5\n",
    "regType = 'l1'\n",
    "includeIntercept = True\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType\n",
    "def parse(line):\n",
    "    print(line)\n",
    "    click = line.click\n",
    "    print(type(click))\n",
    "    c = eval(click)\n",
    "    print(c,type(c))\n",
    "    \n",
    "    h = line.features\n",
    "    print(type(h),h)\n",
    "  \n",
    "    #yield LabeledPoint(c,h)\n",
    "    #sp = SparseVector(7,h)\n",
    "    #yield LabeledPoint(c,h)\n",
    "    print(\"Sp\",h)\n",
    "    yield LabeledPoint(c,h)\n",
    "    \n",
    "#dataRDD = df.flatMap(patse)\n",
    "#dataRDD.take(10)\n",
    "stepSizes = [1,10]\n",
    "\n",
    "f = featurized.select('click','features')\n",
    "f.show(10)\n",
    "f = f.withColumn(\"click\", f[\"click\"].cast(IntegerType()))\n",
    "#xx = f.rdd.flatMap(parse)\n",
    "#xx.take(3)\n",
    "\n",
    "#yy = f.rdd.map(lambda row: LabeledPoint(row.click, row.features))\n",
    "#yy.collect()\n",
    "\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#df = MLUtils.convertVectorColumnsFromML(f, \"features\")\n",
    "rdd = xx\n",
    "#model = LogisticRegressionWithSGD.train(rdd, numIters, stepSize, regParam=regParam, regType=regType,\n",
    "#                       intercept=includeIntercept)\n",
    "#print(model)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"click\",featuresCol=\"features\",maxIter = 10)\n",
    "lrModel = lr.fit(f)\n",
    "print(\"lrmodel\",lrModel)\n",
    "\n",
    "predictions = lrModel.transform(ftestDF)\n",
    "predictions.show(20)\n",
    "\n",
    "testDF.show(20)\n",
    "\n",
    "predictions.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: string, prediction: double, probability: vector, banner_pos: string, site_id: string, rawPrediction: vector, click: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+----------+--------+--------------------+-----+\n",
      "|    hour|prediction|         probability|banner_pos| site_id|       rawPrediction|click|\n",
      "+--------+----------+--------------------+----------+--------+--------------------+-----+\n",
      "|14103023|       0.0|[0.94919308384090...|         0|85f751fd|[2.92757974805002...|    0|\n",
      "|14103023|       0.0|[0.94269475129046...|         0|83a0ad1a|[2.80035031060445...|    1|\n",
      "|14103023|       0.0|[0.97382880887425...|         1|856e6d3f|[3.61657629684801...|    0|\n",
      "|14103023|       0.0|[0.94919308384090...|         0|85f751fd|[2.92757974805002...|    0|\n",
      "|14103023|       0.0|[0.94919308384090...|         0|85f751fd|[2.92757974805002...|    0|\n",
      "|14103023|       0.0|[0.90015883233146...|         1|e151e245|[2.19899062817109...|    1|\n",
      "|14103023|       0.0|[0.94919308384090...|         0|85f751fd|[2.92757974805002...|    0|\n",
      "|14103023|       0.0|[0.97350121606086...|         1|f61eaaae|[3.60380023112514...|    0|\n",
      "|14103023|       0.0|[0.94919308384090...|         0|85f751fd|[2.92757974805002...|    1|\n",
      "|14103023|       0.0|[0.91338437509360...|         0|1fbe01fe|[2.35567656882702...|    0|\n",
      "+--------+----------+--------------------+----------+--------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- C1: string (nullable = true)\n",
      " |-- banner_pos: string (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- click: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = predictions.select(\"hour\", \"prediction\", \"probability\", \"banner_pos\", \"site_id\",\"rawPrediction\",\"click\")\n",
    "display(selected)\n",
    "selected.count()\n",
    "selected.show(10)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "predictions.printSchema()\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"click\")\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "evaluator.getMetricName()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: click)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(f)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "# Use test set to measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(ftestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`REMINDER:`__ If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6232065549203535"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Intercept:  -1.6422745006123267\n"
     ]
    }
   ],
   "source": [
    "print('Model Intercept: ', cvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Feature Weight: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Feature Weight|\n",
      "+--------------+\n",
      "|           0.0|\n",
      "|           0.0|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = cvModel.bestModel.coefficients\n",
    "weights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\n",
    "weightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\n",
    "display(weightsDF)\n",
    "weightsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[click: int, prediction: double, probability: vector, banner_pos: string, C1: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+----------+----+\n",
      "|click|prediction|         probability|banner_pos|  C1|\n",
      "+-----+----------+--------------------+----------+----+\n",
      "|    0|       0.0|[0.88865373183856...|         0|1005|\n",
      "|    1|       0.0|[0.88288527280335...|         0|1005|\n",
      "|    0|       0.0|[0.92815135327597...|         1|1005|\n",
      "|    0|       0.0|[0.88865373183856...|         0|1005|\n",
      "|    0|       0.0|[0.88865373183856...|         0|1005|\n",
      "|    1|       0.0|[0.83826460908098...|         1|1005|\n",
      "|    0|       0.0|[0.88865373183856...|         0|1005|\n",
      "|    0|       0.0|[0.92799432443445...|         1|1005|\n",
      "|    1|       0.0|[0.88865373183856...|         0|1005|\n",
      "|    0|       0.0|[0.84966603496740...|         0|1005|\n",
      "+-----+----------+--------------------+----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"click\", \"prediction\", \"probability\", \"banner_pos\", \"C1\")\n",
    "display(selected)\n",
    "selected.count()\n",
    "selected.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Question Formulation \n",
    "Introduce the goal of your analysis. What questions will you seek to answer, why do people perform this kind of analysis on this kind of data? Preview what level of performance your model would need to achieve to be practically useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Algorithm Explanation\n",
    "Create your own toy example that matches the dataset provided and use this toy example to explain the math behind the algorithym that you will perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: EDA & Discussion of Challenges\n",
    "Determine 2-3 relevant EDA tasks that will help you make decisions about how you implement the algorithm to be scalable. Discuss any challenges that you anticipate based on the EDA you perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_CTR = projectRDD.map(lambda x: int(x.split('\\t')[0])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25622338372976045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_CTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Algorithm Implementation \n",
    "Develop a 'homegrown' implementation of the algorithn, apply it to the training dataset and evaluate your results on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Application of Course Concepts\n",
    "Pick 3-5 key course concepts and discuss how your work on this assignment illustrates an understanding of these concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
