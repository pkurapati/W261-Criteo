{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteo Click Through Rate Prediction \n",
    "Final project for W261 - Machine Learning at Scale  \n",
    "By Ben Arnoldy, Kenneth Chen, Nick Conidas, Rohini Kashibatla, Pavan Kurapati\n",
    "\n",
    "Criteo is a company that uses machine learning to serve up web advertising. In 2014, the company launched the [Click Through Rate (CTR) prediction competition](https://www.kaggle.com/c/criteo-display-ad-challenge) hosted on Kaggle. The group with the lowest loss score won. Our team is working on this challenge for our W261 class at UC Berkeley in order to showcase a semester of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Question Formulation  \n",
    "_Introduce the goal of your analysis. What questions will you seek to answer, why do people perform this kind of analysis on this kind of data? Preview what level of performance your model would need to achieve to be practically useful._\n",
    "\n",
    "The overall question we are trying to answer is, given a set of information about a particular website visitor, session, ad, etc., will that visitor click the ad we serve?  \n",
    "  \n",
    "A company like Criteo could then use the answer to that question to decide which ad out of a series of possible ads to serve in a particular location during a particular session. Companies like Criteo are paid by advertisers at some rate per user click, so to maximize revenue, Criteo would serve the ad that maximizes (CTR * pay per click). If we can predict CTR accurately -- which is the goal of this competition -- Criteo can find the ad that will maximize their revenue.  \n",
    "  \n",
    "This has huge implications because online advertising is a giant industry.  \n",
    "  \n",
    "Spending on digital advertising globally is projected to be $327.28 billion in 2019, according to [eMarketer](https://www.emarketer.com/content/global-ad-spending-update).  \n",
    "  \n",
    "In order for this machine learning prediction to be useful, it must meet a few criteria:  \n",
    "  \n",
    "1. The prediction must happen in a split second. No one wants to wait for webpages to load but this prediction must happen at load time. So algorithm must ingest all the relevant feature data -- info about the user, the session, the ad, etc. -- plug it into the model and return a verdict, and then do that across all potential ads the company can serve, then pick the ad that will maximize revenue and serve it on the page. The pace required immediately rules out learning algorithms like K-Means where you can't pre-train a model. \n",
    "2. For companies like Criteo that are serving ads, it doesn't matter all that much _why_ certain ads have higher CTR; it's just crucial to know _which_ ads will. So, interpretability of the model isn't critical here, opening up the range of algorithms to things like neural nets and random forests. Criteo's customers, of course, would love to have an interpretable model so they could make more effective web ads, but that's a different use case.  \n",
    "3. Web advertising companies are serving a lot of web ad impressions, and most of the time ads go unclicked. And there are a lot of possible features to track in the online space. So we need a lot of data to find something meaningful out of the high-dimensional and sparse data. Fortunately, we have it: the Criteo dataset for this competition has 45 million rows and represents just a week's worth of ad serves. Unfortunately, data on this scale is challenging to work with.\n",
    "4. Whatever model we train will be quickly out of data given the rapid pace of changes in Internet behavior and the growing amount of data that can be used to improve models. That said, it's not clear that the model needs to be an online model where things are changing so fast that it must update instantly as more data streams in. The model training could happen offline in a matter of hours, even days, and still be okay. DO PEOPLE AGREE WITH THIS LAST POINT? I'M NOT SURE.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Algorithm Explanation\n",
    "_Create your own toy example that matches the dataset provided and use this toy example to explain the math behind the algorithm that you will perform._  \n",
    "\n",
    "Logistic regression carries over some of the same concepts as linear regression but with some important differences. Fundamentally, we are still training a model using the equation for a line, and the model is the set of coefficients or weights. \n",
    "\n",
    "\\begin{equation}\\tag{2.1}\n",
    "y = \\beta_0 + \\beta_1 \\cdot x_1 ... + \\beta_m \\cdot x_m\n",
    "\\end{equation}\n",
    "  \n",
    "\\begin{equation}\\tag{2.2}\n",
    "w = [\\beta_0, \\beta_1, ... \\beta_m]\n",
    "\\end{equation}  \n",
    "  \n",
    "where $m$ is number of features, $\\beta_0$ is the intercept or bias term, and $w$ are the weights of our model.  \n",
    "  \n",
    "We aren't training on just one sample, of course, but over many, so the weights are actually calculated over many rows $x$'s and $y$'s.   \n",
    "  \n",
    "\\begin{equation}\\tag{2.3}\n",
    "y_j = \\displaystyle\\sum_{i=1}^{m}{w_i\\cdot x_{ji} + b}\n",
    "\\end{equation}  \n",
    "  \n",
    "where $b$ is the intercept of bias term.  \n",
    "\n",
    "In vector notation, this can be written:  \n",
    "\n",
    "\\begin{equation}\\tag{2.4}\n",
    "\\mathbf{y_j} = \\displaystyle{\\mathbf{w}^T\\mathbf{x}_{j} + b}\n",
    "\\end{equation}  \n",
    "  \n",
    "And we can simplify further by including the bias term b into the weights and augmenting the $\\mathbf{x}$ vector with a value of 1 that will pass through the bias term as b * 1:\n",
    "\n",
    "\\begin{equation}\\tag{2.5}\n",
    "\\mathbf{x}' := \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x}\\\\\n",
    "1\n",
    "\\end{bmatrix},\\quad\n",
    "\\boldsymbol{\\theta} :=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}\\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "\\end{equation}  \n",
    "  \n",
    "leaving us with more compact vector equation for the line:  \n",
    "  \n",
    "\\begin{equation}\\tag{2.6}\n",
    "h(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^T\\mathbf{x}'\n",
    "\\end{equation}\n",
    "  \n",
    "As with linear regression, we use this line to make predictions. In linear regression, we take a new datapoint that has a vector of x-values ($\\mathbf{x}'$), plug it into the linear equation with our model coefficients ($\\boldsymbol{\\theta}$), and get a predicted value h(\\boldsymbol{\\theta}) that falls on the line.  \n",
    "  \n",
    "With logistic regression, however, that isn't our final answer. We aren't just trying to get some continuous value y coordinate for our x, we are trying to arrive at a binary classification of a 1 or a 0. \n",
    "\n",
    "To do that we must do two additional steps. First, we want to contrain our $h(\\boldsymbol{\\theta})$ value to the range of $[0,1]$ and do so in a way groups the data more heavily toward either 0 or 1. We do that through a sigmoid function, whose output can be charted like this ([chart source](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"sigmoid.png\", width=\"400\", height=\"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for the sigmoid is as follows:  \n",
    "\n",
    "\\begin{equation}\\tag{2.7}\n",
    "P(class=1) = \\frac{1} {1 + e^{-z}}\n",
    "\\end{equation}  \n",
    "\n",
    "where $P(class=1)$ is the probability that our predicted value $z$ should be classified as 1. Again, $z$ is our predicted value, so it can thought of as  $h(\\boldsymbol{\\theta})$ or $\\boldsymbol{\\theta}^T\\mathbf{x}'$. So we can write this sigmoid function another way:   \n",
    "  \n",
    "\\begin{equation}\\tag{2.8}\n",
    "P(class=1) = \\frac{1} {1 + e^{-\\boldsymbol{\\theta}^T\\mathbf{x}'}}\n",
    "\\end{equation}\n",
    "  \n",
    "Equation 2.8 formed the basis for our sigmoid function that we wrote for our toy dataset in section 3 below.  \n",
    "  \n",
    "Once we have these probabilities, we need to pick a decision boundary between 0 and 1. One logical default value for that boundary is 0.5. So if the probability is greater than 0.5, classify the sample as 1, otherwise classify it as 0. In chart form, that would look like ([chart source](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"sigmoid_w_threshold.png\", width=\"400\", height=\"200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, in mathemathical form, it looks like:  \n",
    "\\begin{equation}\\tag{2.9}\n",
    "p \\geq 0.5, class=1 \\\\\n",
    "p < 0.5, class=0\n",
    "\\end{equation}\n",
    "\n",
    "Equation 2.9 formed the basis for our decision_boundary function that we wrote in section 3 for our toy dataset.  \n",
    "  \n",
    "Like with linear regression, in order to parallelize the training of the linear model we can't do a closed-form solution but must treat it as an optimization problem with gradient descent. Fortunately, the Log Loss or Cross-Entropy Loss gives us a convex equation, allowing us to use gradient descent with the confidence of not hitting local minima.   \n",
    "  \n",
    "The Log Loss function is much like what it sounds: the negative of the log of the prediction value, though there is two versions depending on the actual label value.  \n",
    "  \n",
    "\\begin{equation}\\tag{2.10}\n",
    "Cost(h_\\theta(x),y) = - log(h_\\theta(x)), if y=1 \\\\\n",
    "Cost(h_\\theta(x),y) = - log(1 - h_\\theta(x)), if y=0\n",
    "\\end{equation}  \n",
    "  \n",
    "We can compile this into a single cost function taking advantage of the fact that we can zero out either side of the equation given $y$ being either 0 or 1 by multiplying one side of the equation by $y$ and another side by $(1-y)$:  \n",
    "\n",
    "\\begin{equation}\\tag{2.11}\n",
    "J(\\theta) = - \\frac{1}{m}\\sum_{i=1}^{m}{[y_i log(h_\\theta(x_i)) + (1 - y_i) log(1 - h_\\theta(x_i))]}\n",
    "\\end{equation}  \n",
    "\n",
    "Using equation 2.6 from above ($h(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^T\\mathbf{x}'$), this can be rewritten as:  \n",
    "\n",
    "\\begin{equation}\\tag{2.12}\n",
    "J(\\theta) = \\frac{1}{m}\\cdot(-\\mathbf{y}\\cdot\\boldsymbol{\\theta}^T\\mathbf{x}' - (1 - \\mathbf{y})\\cdot log(1 - \\boldsymbol{\\theta}^T\\mathbf{x}')\n",
    "\\end{equation}  \n",
    "\n",
    "Equation 2.12 forms the basis of our spark job in the LogLoss function that we built for the toy dataset in section 3 below. \n",
    "\n",
    "For the gradient, we take the first derivative of this cost function, which winds up being:  \n",
    "  \n",
    "\\begin{equation}\\tag{2.13}\n",
    "\\nabla_\\theta f = \\mathbf{x}'\\cdot(h(\\boldsymbol{\\theta}) - \\mathbf{y}) \\\\\n",
    "\\nabla_\\theta f = \\mathbf{x}'\\cdot(\\frac{1}{1+e^{-\\boldsymbol{\\theta} \\mathbf{x'}}} - \\mathbf{y}) \\\\\n",
    "\\nabla_\\theta f = \\frac{x}{1+e^{-\\boldsymbol{\\theta} \\mathbf{x'}}} -\\mathbf{x}'\\mathbf{y} \\\\\n",
    "\\nabla_\\theta f = (\\frac{1}{1+e^{-\\mathbf{y}\\boldsymbol{\\theta} \\mathbf{x'}}} - 1)\\cdot \\mathbf{x}'\\mathbf{y}\n",
    "\\end{equation}  \n",
    "  \n",
    "Equation 2.13 forms the basis of our spark job in the GDUpdate_wReg function that we built for the toy dataset in section 3 below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: EDA...... code for RDDs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numbers\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fproj_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into Spark RDD \n",
    "projectRDD = sc.textFile('gs://w261-bucket-pav/notebooks/data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def extractLabel(line):\n",
    "    \"\"\"Mapper to extract labels\"\"\"\n",
    "    label = line[0]\n",
    "    yield label\n",
    "\n",
    "def extractTrain(line):\n",
    "    \"\"\" Extracts train data\"\"\"\n",
    "    train = line[1:]\n",
    "    yield train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string row to individual features and label\n",
    "projectRDD = projectRDD.map(lambda x: x.split('\\t')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the RDD into train, val and test\n",
    "trainRDD,valRDD,testRDD=projectRDD.randomSplit([0.8, 0.1, 0.1],2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... held out 4585453 records for validation, 4586499 records for test and assigned 36668665 for training.\n"
     ]
    }
   ],
   "source": [
    "print(f\"... held out {valRDD.count()} records for validation, {testRDD.count()} records for test and assigned {trainRDD.count()} for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(element):\n",
    "    \"\"\"\n",
    "    Map record_csv_string --> (tuple,of,fields)\n",
    "    \"\"\"\n",
    "    n_elements = len(element)\n",
    "    fields = np.array(element)\n",
    "    features,click = fields[1:], fields[0]\n",
    "    return(features, click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDDCached = trainRDD.map(parse).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract a toy dataset\n",
    "To just show the workings of the algorithm, we want a small number of rows and avoid types of data that we can't handle. That means we need to remove categorical features and NAs. We explored a number of possibilities for handling the NA problem:\n",
    "1. Simply removing all rows with missing data. The problem: That results in the loss of approximately 90 percent of the data. \n",
    "2. Creating dummy variables for each integer feature that would be set to True if the feature had data or False if the feature had no data. Then, we would replace all NAs in the integer features with zeros. So, for I1 there would be an I1_missing, for I2 an I2_missing, and so on. We did not see an accuracy gain from those Ix_missing features and instead saw L1 Normalization zeroing those out. \n",
    "3. Replace the NAs with mean values of each feature. This seemed to strike the right balance between preserving data and not creating unnecessary features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_categorical(x):\n",
    "    features = x[0]\n",
    "    label = x[1]\n",
    "    # We are interested in C6,C9, C14, C17, C20, C22, C23\n",
    "    # These are at index 18,21,26,29,32,34,35\n",
    "    c_index_list = [18,21,26,29,32,34,35]\n",
    "    new_arr = np.array(features[0:13],dtype=str)\n",
    "    for index in c_index_list:\n",
    "        new_arr = np.append(new_arr,features[index])\n",
    "    yield((new_arr,label))\n",
    "\n",
    "new_trainRDDCached = trainRDDCached.flatMap(remove_categorical).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all blanks in new_trainRDDCached integer values with their means\n",
    "\n",
    "def replace_int_null(x):\n",
    "    features = x[0]\n",
    "    label = int(x[1])\n",
    "    for i in range(13):\n",
    "        if features[i]==\"\":\n",
    "            features[i] = float(mean_dict_bc.value[i+1])\n",
    "        else:\n",
    "            features[i] = float(features[i])\n",
    "    features = features.astype(float)\n",
    "    yield((features,label))\n",
    "\n",
    "def replace_int_null_noInt(x):\n",
    "    features = x[0]\n",
    "    label = int(x[1])\n",
    "    for i in range(13):\n",
    "        if features[i]==\"\":\n",
    "            features[i] = float(mean_dict_bc.value[i+1])\n",
    "        else:\n",
    "            features[i] = float(features[i])\n",
    "    yield((features,label))\n",
    "    \n",
    "def replace_char_null(x):\n",
    "    features = x[0]\n",
    "    label = int(x[1])\n",
    "    for i in range(13,len(features)):\n",
    "        if features[i]==\"\":\n",
    "            features[i] = 'AAAA'\n",
    "    yield((features,label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array(['2.0', '0.0', '44.0', '1.0', '102.0', '8.0', '2.0', '2.0', '4.0',\n",
       "         '1.0', '1.0', '0.991518', '4.0', 'fe6b92e5', 'a73ee510',\n",
       "         'b28479f6', '07c540c4', '5840adea', 'AAAA', '3a171ecb'],\n",
       "        dtype='<U8'), 0)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trainRDDCached = new_trainRDDCached.flatMap(replace_int_null_noInt)\\\n",
    "                                    .flatMap(replace_char_null).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  2.      ,   0.      ,  44.      ,   1.      , 102.      ,\n",
       "           8.      ,   2.      ,   2.      ,   4.      ,   1.      ,\n",
       "           1.      ,   0.991518,   4.      ]), 0)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IntFeatRDD = IntFeatRDD.flatMap(replace_int_null).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the toy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (features_array, y)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: (np.divide((x[0] - featureMeans), featureStdev), x[1]))        \n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "normedRDD = normalize(IntFeatRDD).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run logistic regression, include L1, L2 normalization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute loss using log loss or cross entropy formula.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Adapted from sources:\n",
    "    * Homework 4 code for W261\n",
    "    * https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "    * https://www.internalpointers.com/post/cost-function-logistic-regression\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "\n",
    "    loss = augmentedData.map(lambda x: (-np.dot(x[1],np.log(1 / (1 + np.exp(-np.dot(W, x[0]))))) -\\\n",
    "                                        np.dot((1-x[1]),np.log(1- (1 / (1 + np.exp(-np.dot(W, x[0])))))), 1)) \\\n",
    "                        .reduce(lambda x,y:(x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "    loss = float(loss[0])/loss[1]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate_wReg(dataRDD, W, N, learningRate = 0.1, regType = None, regParam = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent step/update with ridge or lasso regularization.\n",
    "    Args:\n",
    "        dataRDD - tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        learningRate - (float) defaults to 0.1\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient\n",
    "    Returns:\n",
    "        model   - (array) updated coefficients, bias still at index 0\n",
    "    Adapted from sources:\n",
    "    * Homework 4 code from W261\n",
    "    * Notebook cited by Jimi in asynch: https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/r20ff7q0yni5kiu/LogisticRegression-Spark-Notebook.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    # augmented data\n",
    "    # this puts a 1.0 in front of x's to pass thru bias term\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])) \n",
    "    \n",
    "    new_model = None\n",
    "    \n",
    "    # calculate the gradient\n",
    "    \n",
    "    gradient = augmentedData.map(lambda x: (1 / (1 + np.exp(-x[1]*np.dot(W, x[0])))-1) * x[1] * np.array(x[0]))\\\n",
    "                    .reduce(lambda x, y: x + y)\n",
    "    if regType == \"ridge\":\n",
    "        wReg = W * 1\n",
    "        wReg = np.append([0], wReg[1:]) # remove the bias term ahead of regularization        \n",
    "    elif regType == \"lasso\":\n",
    "        wReg = W * 1\n",
    "        wReg = np.append([0], wReg[1:]) # remove the bias term ahead of regularization\n",
    "        wReg = (wReg>0).astype(int) * 2-1\n",
    "    else:\n",
    "        wReg = np.zeros(W.shape[0])\n",
    "    gradient = gradient + regParam * wReg  #gradient:  GD of Squared Error+ GD of regularized term \n",
    "    new_model = W - learningRate * gradient / N\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n",
    "                         regType = None, regParam = 0.1, verbose = True):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of regularized gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    Adapted from sources:\n",
    "    * Homework 4 code from W261\n",
    "    * Notebook cited by Jimi in asynch: https://nbviewer.jupyter.org/urls/dl.dropbox.com/s/r20ff7q0yni5kiu/LogisticRegression-Spark-Notebook.ipynb    \n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # calculate N here so you don't have to do it in the for looped function call\n",
    "    N = trainRDD.count()\n",
    "\n",
    "    # make a starter set of weights if none provided\n",
    "    featureLen = len(trainRDD.take(1)[0][0])\n",
    "    if wInit is None:\n",
    "        w = np.random.normal(size=featureLen) # w should be broadcasted if it is large\n",
    "    else:\n",
    "        w = wInit\n",
    "    model = w\n",
    "    \n",
    "    # perform nSteps updates & compute test and train loss after each\n",
    "    for idx in range(nSteps):  \n",
    "   \n",
    "        # update the model\n",
    "        model = GDUpdate_wReg(trainRDD, model, N, learningRate, regType, regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        training_loss = LogLoss(trainRDD, model)\n",
    "        test_loss = LogLoss(testRDD, model)\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and train a logistic regression model with L1 regularization\n",
    "\n",
    "# initialize the weights with a bias term determined after some testing and zeros for coeffecients\n",
    "wInit = np.array([-0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "trainRDD, testRDD = normedNONARDD.randomSplit([0.8,0.2], seed = 5)\n",
    "start = time.time()\n",
    "lasso_results = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 10, regParam = 0.1)\n",
    "print(f\"\\n... trained {len(lasso_results[2])} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make error curve plots that show declining loss as walk down the gradient\n",
    "# from HW4 code\n",
    "\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of log loss) , testLoss (list of log loss)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Log Loss')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv files\n",
    "# from HW4 code\n",
    "\n",
    "trainLoss, testLoss, models = lasso_results\n",
    "np.savetxt(PWD + '/data/lasso_models.csv', np.array(models), delimiter=',')\n",
    "np.savetxt(PWD + '/data/lasso_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')\n",
    "plotErrorCurves(trainLoss, testLoss, title = 'Lasso Regression Error Curves' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model from files\n",
    "# from HW4 code\n",
    "\n",
    "saved_models = np.loadtxt(PWD + '/data/lasso_models.csv', dtype=float, delimiter=',')\n",
    "best_model = saved_models[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the coeffecients as L1 steps increase\n",
    "# adapted from HW4 code\n",
    "\n",
    "def plotCoeffs(models, featureNames, title):\n",
    "    \"\"\"\n",
    "    Helper Function to show how coefficients change as we train.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize = (15,8))\n",
    "    X = list(range(len(models)))\n",
    "    for data, name in zip(models.T, featureNames):\n",
    "        if name == \"Bias\":\n",
    "            continue\n",
    "        ax.plot(X, data, label=name)\n",
    "    ax.plot(X,[0]*len(X), 'k--')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show\n",
    "    \n",
    "#use if starting with the models with the missing data features\n",
    "#plotCoeffs(saved_models, ['Bias'] + numeric_features + missing, \"Lasso Coefficients over 50 GD steps\")\n",
    "\n",
    "# use if starting with the model where NA rows were dropped\n",
    "plotCoeffs(saved_models, ['Bias'] + numeric_features, \"Lasso Coefficients over 50 GD steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy of the model\n",
    "# from learning here: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "def sigmoid(row):\n",
    "    sig = []\n",
    "    for z in row:\n",
    "        sig.append(1/(1+np.exp(-z)))\n",
    "    return sig\n",
    "\n",
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= .5 else 0\n",
    "\n",
    "def classify(predictions):\n",
    "    '''\n",
    "    input  - N element array of predictions between 0 and 1\n",
    "    output - N element array of 0s (False) and 1s (True)\n",
    "    '''\n",
    "    classifications = []\n",
    "    for prediction in predictions:\n",
    "        classifications.append(decision_boundary(prediction))\n",
    "    return classifications\n",
    "\n",
    "def calc_accuracy(predicted_labels, actual_labels):\n",
    "    error_count = 0\n",
    "    num_predictions = len(actual_labels)\n",
    "    for compare in range(num_predictions):\n",
    "        if predicted_labels[compare] != actual_labels[compare]:\n",
    "            error_count += 1\n",
    "    return 1.0 - (float(error_count) / num_predictions)\n",
    "\n",
    "def compare(x,y):\n",
    "    return int(x != y)\n",
    "\n",
    "coeffs = best_lasso[1:] # coefficients\n",
    "b = best_lasso[0] # bias term\n",
    "actual_labels = testRDD.map(lambda x: x[1]).collect()\n",
    "\n",
    "# regular version -- good for toy example, may need spark below on big data\n",
    "mxb = testRDD.map(lambda x: np.dot(coeffs, x[0])+b).collect() # mx+b\n",
    "probabilities = sigmoid(mxb) # put through sigmoid\n",
    "predictions = classify(probabilities) # put through a decision boundary\n",
    "print (\"accuracy is\", calc_accuracy(predictions, actual_labels))\n",
    "\n",
    "# spark version below if regular version crashes with big data\n",
    "\"\"\"\n",
    "error_count = testRDD.map(lambda x: (np.dot(coeffs, x[0])+b, x[1])) \\\n",
    "                    .map(lambda x: (1/(1+np.exp(-x[0])), x[1])) \\\n",
    "                    .map(lambda x: (decision_boundary(x[0]), x[1])) \\\n",
    "                    .map(lambda x: (compare(x[0],x[1]),1)) \\\n",
    "                    .reduce(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
    "\n",
    "print(\"accuracy is\", 1.0 - (float(error_count[0]) / error_count[1]))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
